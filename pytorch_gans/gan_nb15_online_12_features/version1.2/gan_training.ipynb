{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d45dfea-b5ab-4ee0-81db-982a5de12b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch import FloatTensor, LongTensor\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "645898be-0d8e-400b-bf4d-6339c2253208",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_nb15 = ['dsport', 'dur', 'Spkts', 'Dpkts', 'sbytes', 'dbytes', 'smeansz', \n",
    "                          'dmeansz', 'flow_bytes/s', 'flow_packets/s', 'fwd_packets/s', 'bwd_packets/s']\n",
    "\n",
    "selected_features_total = selected_features_nb15\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf915d9a-c8de-4044-9c9b-3f63660fd549",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessor:\n",
    "    def __init__(self, file_path, selected_features):\n",
    "        self.input_dataset = pd.read_csv(file_path)\n",
    "        self.selected_features = selected_features\n",
    "        \n",
    "    def print_input_data(self):\n",
    "        print('The input dataset has the following columns/features')\n",
    "        print(self.input_dataset.columns)\n",
    "        labels = self.input_dataset['Label']\n",
    "        print('The labels are as follows')\n",
    "        print(labels.value_counts())\n",
    "        print(labels.value_counts().plot.pie())\n",
    "        \n",
    "    def preprocess(self):\n",
    "        X, y = self.feature_selection()\n",
    "        X = self.imputing(X)\n",
    "        X = self.scaling(X)\n",
    "        y = self.label_binarization(y)\n",
    "        self.data = torch.tensor(X)\n",
    "        self.targets = torch.tensor(y)\n",
    "        \n",
    "    def feature_selection(self):\n",
    "        X = self.input_dataset[self.selected_features]\n",
    "        y = self.input_dataset['Label']\n",
    "        return X, y\n",
    "    \n",
    "    def imputing(self, X):\n",
    "        X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        Imp = SimpleImputer()\n",
    "        X = Imp.fit_transform(X)\n",
    "        return X\n",
    "        \n",
    "    def scaling(self, X):\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "        joblib.dump(scaler, 'minmaxscaler')\n",
    "        return X\n",
    "    \n",
    "    def label_binarization(self, y):\n",
    "        for label in y:\n",
    "            if (label != 'BENIGN' and label != 'ATTACK'):\n",
    "                y.replace(label, 'ATTACK', inplace=True)\n",
    "        y.replace('BENIGN', 1, inplace=True)\n",
    "        y.replace('ATTACK', 0, inplace=True)\n",
    "        return y\n",
    "    \n",
    "    def return_data(self):\n",
    "        return self.data, self.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b3352fd-24e3-418b-ad69-5610a96dfc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_path, selected_features):\n",
    "        preprocessor = PreProcessor(file_path, selected_features)\n",
    "        preprocessor.print_input_data()\n",
    "        preprocessor.preprocess()\n",
    "        self.data, self.targets = preprocessor.return_data()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data[idx]\n",
    "        label = self.targets[idx]\n",
    "        return row, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90b14a53-341f-4950-9ebe-40901a12929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0319c25f-9fc6-4081-8431-c128778ea25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input dataset has the following columns/features\n",
      "Index(['dsport', 'dur', 'Spkts', 'Dpkts', 'sbytes', 'dbytes', 'smeansz',\n",
      "       'dmeansz', 'flow_bytes/s', 'flow_packets/s', 'fwd_packets/s',\n",
      "       'bwd_packets/s', 'Label'],\n",
      "      dtype='object')\n",
      "The labels are as follows\n",
      "BENIGN    354996\n",
      "ATTACK     51411\n",
      "Name: Label, dtype: int64\n",
      "Axes(0.22375,0.11;0.5775x0.77)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katsa\\AppData\\Local\\Temp\\ipykernel_4444\\1543300877.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.replace([np.inf, -np.inf], np.nan, inplace=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaoAAAGFCAYAAACsdbcVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAvJUlEQVR4nO3dd3hUVcIG8HeSSe+FkEooCYEAEZSmIpIgJIgoRUAEBUF3XRXrirhIsWBbC9bdFULzQwUWVEBFBHQBUSBCqAFSCIE00pOZ1CnfH2gkkDaTmTlnZt7f88xDSG7uvME479x7zz1Hodfr9SAiIpKUg+gARERErWFRERGR1FhUREQkNRYVERFJjUVFRERSY1EREZHUWFRERCQ1FhUREUmNRUVERFJjURERkdRYVEREJDUWFRERSY1FRUREUmNRERGR1FhUREQkNRYVERFJjUVFRERSY1EREZHUWFRERCQ1FhUREUmNRUVERFJjURERkdRYVEREJDUWFRERSY1FRUREUmNRERGR1FhUREQkNRYVERFJjUVFRERSY1EREZHUWFRERCQ1FhUREUmNRUVERFJjURERkdRYVEREJDUWFRERSY1FRUREUmNRERGR1FhUREQkNRYVERFJjUVFRERSY1EREZHUWFRERCQ1FhUREUmNRUVERFJTig5AJDO9Xg91vRZVtQ2oqtWgqrYBlbWaxo+b/qlBg1YHZ6UDXJQOcHZ0gLPy94ej458fKx3g8vvXXJQOCPRyQYiPK4K9XaF05HtHoquxqMiu1Wt0uFBWjZzSalz4/XH54xrkVdSgsqYBOr1lsjgogE5eLgjxcUOorytCfdwQ4uuGUB/Xxj8DPV3g4KCwTCAiSSj0er2F/jckEqemXoszhVXIuKRCZpHq8p+XVMgprYbGUk1kAk6OCoT7uaNXsBdiQ7wRG+qN3iHeCPV1Ex2NyGxYVGSTiqrqkJJdikPZZUg5X4pTeZVWVUiG8nN3QmyoN64L98WALn7oH+GLTl4uomMRmQSLimxCZpHqz2LKLkV2SbXoSMKF+bphQJfLxTU8OhDRnb1ERyIyCouKrNLxixU4cK4EB8+V4nBOGYpV9aIjSS/czw3xMUFI6BWEG3sEwNXJUXQkonZhUZFV0Ov1SDlfhm+P5+P7EwXIq6gVHcmquTo54KYegYiP6YT4XkEI93MXHYmoRSwqkpZWp8eBcyX47ngBvj9ZgEtVdaIj2ayenT0R3ysICTFBuCHSj8PkSSosKpKKRqvDz5kl2H4iHztOFqJEzVN6lubtqsTt/UIwdVAEBnTxEx2HiEVF4ul0evwvvQjbjuZjZ1ohKmoaREei38V09sKUQRGYOCAMfh7OouOQnWJRkTAV1Q1Yn5KDT389jwulNaLjUCucHR0wKrYzpg6KwLCoQN50TBbFoiKLO11QiTX7s/HVkTzUNGhFxyEDhfm6YfLAcEweGIEw3mhMFsCiIovQaHXYcaoQq/dn4+C5UtFxyAQcFMDNUYGYNrgLEvsEw5FHWWQmLCoyqxJVHT4/mIN1B3KQzyHlNqtboAceGdEDEwaEccQgmRyLisziVF4lVuzLwrZj+ajX6ETHIQuJ8HfDIyOicPcN4XBiYZGJsKjIpNILq/DOD2ex/WQB+Jtlv8J83fDwrd0xZVAEXJScAYM6hkVFJpFVpMJ7u9Kx9WiexZbFIPkFe7viL8O7494hXThlExmNRUUdklteg3d/OIsvj+RCy4aiFgR6uuAvw7thxtBIuDtzGTwyDIuKjFJR04CPf8zAqv3ZvAZF7ebv4YzH4qNw/42RHHRB7caiIoPUa3RY+0s2PvwxA+XVnEGCjBMd5IlF42JxS3Qn0VHICrCoqN2+PZ6P175L4ywSZDKjYjtj4dhYdAng7O3UMhYVtamgohYvfHUCO9MKRUchG+SsdMBDt3TD3IRoDrigZrGoqEV6vR6fHczB69+eRlWdRnQcsnFd/N3x0l19MCImSHQUkgyLipp1rliN+ZuO4QCnOyILGxsXgsV3xCLI21V0FJIEi4qa0Gh1WL73HJbtPIs6juYjQbxclXg2MQYzhkRypnZiUdGfTuRW4LlNx3Ayr1J0FCIAwM1RAXh3Sn8eXdk5FhWhtkGL93alY/meLGh40y5Jxt/DGW9NjkNCr86io5AgLCo7d/RCOZ5an4qsYrXoKEStmnVTVzx/ey/OHWiHWFR27NNfsvHytjTUa3ktiqxDbIg33p82AFFBnqKjkAWxqOxQTb0W//jyOL48kis6CpHB3JwcsXhcLO4Z3EV0FLIQFpWdOVesxt/+7zecLqgSHYWoQ8bGheC1if3g7eokOgqZGYvKjmw/UYBnNx7lzbtkM8L93PDePQNwQ6Sf6ChkRiwqO6DV6fHG9tP4ZE+W6ChEJqd0UGD+mF548JbuoqOQmbCobNylqlrM/ewIZ5ggm3ff0EgsubMPHHmDsM1hUdmwg+dK8dhnh3Gpqk50FCKLSOgVhA/vHcDFGW0Mi8pGrT+UgwVfnuANvGR3+oR6Y+WsQejM2SxsBovKBn38Uwbe3H5GdAwiYUJ9XLHygUHoFewtOgqZAIvKxrz6bRoHTRAB8HJR4qPp12N4T64ibO1YVDZCq9Nj/qZj2PjbRdFRiKShdFDglfF9eXOwlWNR2YDaBi3mfn4EP5ziCrxEzXlkRA88mxgDhYIjAq0Ri8rKVdU24KG1Kfg1i8PPiVoz7rpQvDvlOigdHURHIQOxqKxYiaoOM1cdxIlcrh9F1B5j+4Xg/WkDeK+VleHNBlbqYlk17k8+yOU5iAzwzfF8ODkq8M6U/lw52IqwqKxQemEV7ks+iILKWtFRiKzOV6l5cHJ0wJt3x/GalZXgyVork12sxr0rDrCkiDpg428XseCrE6JjUDuxqKxIXnkNpq84gCJOiUTUYZ8dyMGSLSdFx6B2YFFZiWJVHWYkH0BueY3oKEQ2Y/X+bCz95pToGNQGFpUVqKhpuDxwoogDJ4hMbfnec/jn96dFx6BWsKgkV9ugxZzVh3Aqn0PQiczlox8z8d7OdNExqAUsKonpdHo8/vkRpJwvEx2FyOa9u/Ms/v2/TNExqBksKokt3nISOzgtEpHFvLH9NLYezRMdg67CopLURz9m4NNfz4uOQWRX9Hrg7xuPIvVCuegodAUWlYQ2H76If37P9aSIRKjT6PDQ2hTkcYStNFhUkknJLsVzm46JjkFk14qq6jBnTQrUdRrRUQgsKqkUq+rw6GeH0aDlPMFEoqXlV+KJL1LBebvFY1FJQqfT44kvjqCwkrNOEMliZ1ohlnHYunAsKkm888NZ/JxRIjoGEV3l/d3p2MnRt0KxqCTw4+lL+OinDNExiKgZej3w1PpUZBWpREexWywqwS6WVeOpDangaXAieVXVafCXT3+DioMrhGBRCVSv0eHRdYdRXt0gOgoRtSHjkgrz/ntUdAy7xKIS6KVtJ3H0YoXoGETUTt8eL8Cm3y6KjmF3WFSCfJ2ai//7NUd0DCIy0JKtJ3kzsIWxqARIL6zC85uPi45BREaoqtVg3n+P8f4qC2JRWVi9RodHPzuM6nqt6ChEZKR9GcVY+wvn4rQUFpWFffRjBs4WcpgrkbV7/bvTOFfMxUwtgUVlQemFVfjXT1zvhsgW1DRo8fSGVGh1PAVobiwqC9Hr9Zi/+TjqtTrRUYjIRI7klHOxRQtgUVnIp7+ex29cqZfI5ry3Mx2n8ipFx7BpLCoLyK+owZvbub4UkS2q1+rw9IZU1Gt4tsRcWFQWsPCrE5x6hciGnS6owjs/nBUdw2axqMxs27E87Ey7JDoGEZnZ8r1ZOFtYJTqGTWJRmVFFdQOWbDklOgYRWYBWp8cr36SJjmGTWFRmtPTbUyhWcSFEInux52wRfjrDMyimxqIyk/2ZxdiQwskriezN0m/SoOFtKCbFojIDvV6Pl7bylB+RPUq/pMLnBznhtCmxqMxgy9E8nC7gRVUie/XuznRU1nKdOVNhUZmYRqvjMFUiO1eqrseHuzNEx7AZLCoTW59yAedLqkXHICLBVv+cjRy+FpgEi8qEahu0+GAX30UR0eUZK177jsPVTYFFZUJrf8lGQWWt6BhEJInvThTgQFaJ6BhWz+CimjVrFhQKReMjICAASUlJOHbsWOM2V379yscXX3wBAPjpp5+gUCjQt29faLVNFxD09fXF6tWrG//etWtXLFu2rMk2R44cwdSpUxESEgIXFxdERkbijjvuwNatWxtX3czOzoZCoUBQUBCqqpoObOjfvz+WLFli6I/eKlWdhkt4ENE1XvkmjasBd5BRR1RJSUnIz89Hfn4+du3aBaVSiTvuuKPJNqtWrWrc5o/H+PHjm2yTmZmJtWvXGvTcX3/9NYYOHQqVSoU1a9bg1KlT2LhxI8aPH48XXngBFRUVTbavqqrCW2+9ZcyPaZDle7JQVs1RPkTU1PHcCnx/slB0DKtmVFG5uLggODgYwcHB6N+/P5577jlcuHABRUVFjdv4+vo2bvPHw9XVtcl+5s6di8WLF6O2tn2ny9RqNebMmYOxY8fim2++wejRo9GjRw8MHjwYDz74II4ePQofH59rnuOdd97BpUvmu1u8VF2P5H3nzLZ/IrJun+zh2ZaO6PA1KpVKhXXr1iEqKgoBAQEGfe+TTz4JjUaDDz/8sF3b79ixAyUlJZg3b16L2ygUiiZ/nzZtGqKiovDSSy8ZlM0QH/+YwdnRiahFh3PKkZJdKjqG1TKqqLZt2wZPT094enrCy8sLW7Zswfr16+Hg8Ofupk2b1rjNH4+srKwm+3F3d8fixYvx2muvXXPKrjlnz16+PykmJqbxc4cOHWryHNu2bWvyPQqFAq+//jo++eQTZGaa/l1NfkUNPv31vMn3S0S25T97streiJplVFHFx8cjNTUVqampOHDgAEaPHo0xY8bg/Pk/X7Dffffdxm3+eERERFyzrzlz5iAwMBBvvPGGUT9AXFxc4/7VajU0mmuPbBITEzFs2DAsXLjQqOdozX/+l4U6LphGRG3YmVaIzCKV6BhWyaii8vDwQFRUFKKiojB48GAkJydDrVZj+fLljdsEBwc3bvPHw8nJ6Zp9KZVKvPLKK3jvvfeQl5fX6vNGR0cDAM6c+XO1XBcXl8b9t+b111/H+vXrceTIEUN+1FZV1jZgY8oFk+2PiGyXXg+s2MujKmOY5D4qhUIBBwcH1NTUGPX9kydPRp8+ffDiiy+2ut3o0aPh7+9v1NHX4MGDMXHiRMyfP9+ojM3ZcOgC1PXatjckIgKw6XAuiqq49I+hlMZ8U11dHQoKCgAAZWVl+PDDD6FSqTBu3LjGbcrLyxu3+YOXlxc8PDya3efrr7+OxMTEVp/X09MTK1aswNSpUzF27Fg8/vjjiI6Ohkqlwvbt2wEAjo6OLX7/0qVL0adPHyiVRv3YTeh0eqz5JbvD+yEi+1Gv0WHN/mz8PTGm7Y2pkVFHVNu3b0dISAhCQkIwZMgQHDp0CBs3bsSIESMat3nggQcat/nj8cEHH7S4z4SEBCQkJDR7jelKEyZMwP79++Hu7o77778fMTExSEhIwO7du/HFF19ccz/XlXr27InZs2e3ezh8a3acKsSFUuOOIInIfv3fgfOorucoYUMo9Lxl2ihT//MLDpzjcFMiMtzicbF44OZuomNYDc71Z4QzBVUsKSIyWvK+c9DqeIzQXiwqI3x2gPdNEZHxLpbV4LsT+aJjWA0WlYFq6rXYfCRXdAwisnIbUi6KjmA1WFQG2nosD1W1vBBKRB2zL70IBRVcFqg9WFQGWncgR3QEIrIBOj2w+QiPqtqDRWWAk3kVOHqhXHQMIrIRm35jUbUHi8oAW1Jbn+KJiMgQmUVqHMkpEx1DeiwqA2w/WdD2RkREBth8mIOz2sKiaqdTeZU4X1ItOgYR2Zhvj+fznqo2sKjaaTvveSAiMyhR12N/ZrHoGFJjUbUTT/sRkblsO8o3wq1hUbVDZpEKZwu54BkRmcf2kwVo0HIB1pawqNph+wkeTRGR+VTUNGBvepHoGNJiUbUDi4qIzO2bY3ydaQmLqg0Xy6pxPLdCdAwisnH7MnhE1RIWVRt4NEVEllBYWYeMS1WiY0iJRdWG7znaj4gs5OeMEtERpMSiakVRVR1+O8/pTYjIMn7O4P1UzWFRtWJ/ZjF4wzgRWcqvWSWcpaIZLKpWpGTzaIqILKeyVsPBW81gUbUihaf9iMjCePrvWiyqFlTVNuBMQaXoGERkZzjv37VYVC04klPO61NEZHEp2WWobdCKjiEVFlULONqPiESo0+j4+nMVFlUL+ItCRKLwOlVTLKpmaHV6Lg9NRML8nMkbf6/EompGWn4l1PU8R0xEYqTlV/J+qiuwqJrB035EJFK9RodzxWrRMaTBomoG758iItHOFHCC2j+wqJpxmEVFRIKdKWRR/YFFdZViVR1yy2tExyAiO3eWR1SNWFRX4XlhIpIBj6j+xKK6SjaLiogkcL5EzRkqfseiusr5kmrREYiIoNMD6YUq0TGkwKK6SnYJj6iISA48/XcZi+oqPKIiIlmcZVEBYFFd4zyPqIhIEqc58g8Ai6qJUnU9Kms1omMQEQHgEPU/sKiuwOtTRCSTgspa1HDeURbVlXjaj4hkU6yqEx1BOBbVFbKLOZCCiOTComJRNcFTf0QkmxJVvegIwrGorsCh6UQkmxI1j6iU7d1wy5Yt7d7pnXfeaVQY0Yqq+AtBRHIp5hFV+4tq/Pjx7dpOoVBAq7XOUSoVNQ2iIxARNcFTfwYUlU6nM2cO4TRaHVR1vIeKiOTCU38muEZVW1trihzC8WiKiGTEIyoji0qr1eLll19GWFgYPD09kZWVBQBYuHAhkpOTTRrQUspZVEQkIQ5PN7Koli5ditWrV+PNN9+Es7Nz4+f79euHFStWmCycJZVXs6iISD4lah5RGVVUa9euxSeffILp06fD0dGx8fNxcXE4ffq0ycJZEq9PEZGMytT10Ov1omMIZVRR5ebmIioq6prP63Q6NDRY55FJTT2Liojko9Hp7f4aulFF1adPH+zdu/eaz2/cuBEDBgzocCgRarjkMxFJqtrOJ6Zt9/D0Ky1evBj33XcfcnNzodPpsHnzZpw5cwZr167Ftm3bTJ3RIuz9F4GI5KXV8dSfwcaNG4f169fj22+/hUKhwKJFi5CWloatW7di1KhRps5oEZxKn4hkZe9FZdQRFQAkJiYiMTHRlFmEquWpPyKSlIZFZbyUlBSkpaVBoVCgd+/euOGGG0yVy+LqNbY98wYRWS+dnY/6M6qoLl68iGnTpuHnn3+Gr68vAKC8vBw33XQTPv/8c0RERJgyo0UoHTmRPBHJSaNlURls9uzZaGhoQFpaGmJiYgAAZ86cwezZszFnzhzs2LHDpCEtwUXJoiLTuTu4EC8pV0Oh5yll6jgHh5UAvEXHEMaootq7dy/279/fWFIAEBMTgw8++AA333yzycJZEouKTOm/BZ0xsPvtmJr3OhSw73fDZAIK+37DY9Src5cuXZq9sVej0SAsLKzDoURwcXJseyMiA8zP6ofNYc9CD4XoKGTtHDo0nMDqGVVUb775JubOnYuUlJTGqT1SUlLwxBNP4K233jJpQEvhERWZwzOZ/bE1/GnRMcjaOdp3USn07ZxEys/PDwrFn+8M1Wo1NBoNlMrL/4B/fOzh4YHS0lLzpDWjb47l49HPDouOQTbq46hDuP3iu6JjkLV68jjg20V0CmHaXdPLli0zYwzxeERF5vRIxiB8EvU4Rl98X3QUskZ2fuqv3T/9zJkzzZlDOBcnFhWZ118yhmJltBYJFz4SHYWsjZ0XVYdfnWtqalBZWdnkYY1clBxMQeY3O/1m7I14WHQMsjaOzm1vY8OMKiq1Wo3HHnsMQUFB8PT0hJ+fX5OHNeKpP7KU+9KH49eIh0THIGvh6Ay4+YpOIZRRr87z5s3D7t278fHHH8PFxQUrVqzAiy++iNDQUKxdu9bUGS2Cp/7Iku5Jj8ehiNmiY5A18OwsOoFwRr06b926FR9//DHuvvtuKJVK3HLLLXjhhRfw6quvYt26dabOaBE89UeWNjn9NhzpYtvXfskEvIJFJxDOqKIqLS1Ft27dAADe3t6Nw9GHDRuGPXv2mC6dBXm72vfFShJjwtlEHI+YLjoGyYxHVMYVVffu3ZGdnQ0AiI2NxYYNGwBcPtLy8fExWThLCvB04XUqEmJc+licipgmOgbJyitEdALhjHplfuCBB3D06FEAwPPPP994reqpp57CvHnzTBrQksJ83URHIDt1e/o4nI2YLDoGyYin/oyblPapp55q/Dg+Ph6nT59GSkoKOnXqhFWrVpksnKWF+rohq1gtOgbZqcSM8dgZpUOPC5tERyGZsKg6fh8VcHmS2okTJ8Lb2xtr1qwxxS6FCPV1FR2B7Jher8BtGRNxLny86CgkExaVaYrKVoTy1B8JptcrcFvm3cgJHyc6CsnCk0XForoCi4pkoNU7YGTWPbgYfrvoKCQDDqZgUV2JgylIFg06BUZm3Yv8sETRUUgkR2fAI0B0CuEMGkwxceLEVr9eXl7ekSzC8YiKZFKnc8CIc/dhT1ctOuftFB2HROA9VAAMLKq27pHy8fHB/fff36FAIoX6ukKhANq3QheR+dXpHJBwfhZ+itSiU96PouOQpXEgBQADi8qah563h4vSEQEeLihW1YmOQtRIrXXAiJzZ2BOhRUC+dc78QkbiERUAXqO6RhiHqJOE1BpHjLjwIMqCbxYdhSzJv5voBFJgUV0lzI/XqUhOVRolRuT+FRWdh4qOQpYS0l90AimwqK7SK9hbdASiFlU0KDEi7xFUdh4sOgpZQugA0QmkwKK6Sly4dU6qS/ajrEGJkfmPoipooOgoZE6uPoB/d9EppMCiukpcuK/oCERtKqp3wsjCuVB36i86CplLyHWAQiE6hRRYVFfx93BGOK9TkRW4VOeE24qeRHVgnOgoZA68PtWIRdUMnv4ja5Ff64zRxU+hJqCv6Chkarw+1YhF1Qye/iNrcrHWBYmlT6PWv7foKGRKof1FJ5AGi6oZPKIia5NT44qx5X9HnV+M6ChkCq6+HEhxBRZVM/qF+fAaJlmdzGo3jKuch3rfKNFRqKN4NNUEi6oZXq5O6BboIToGkcHOqt1wl2o+6n35btyqcSBFEyyqFsSF8fQfWac0lTsmqZ9Hg09X0VHIWBxI0QSLqgUcUEHW7HiVBybX/AMa7y6io5AxeOqvCRZVC66L4BEVWbfUSk9MqVsAjVeY6ChkCDc/wK+r6BRSYVG1oE+oD1yd+M9D1u1whRemNyyE1jNUdBRqr7AbRCeQDl+JW+Dq5Igh3bgENFm/A+XemKlbCK0HF+GzCtGJohNIh0XVilt7dhIdgcgk9pX6YDYWQusRJDoKtSVmjOgE0mFRteLWGBYV2Y7/lfjhL1gEnXug6CjUkuB+gG+E6BTSYVG1okcnT0T4c4Jash27SvzxiONi6Nz8RUeh5sTcLjqBlFhUbRgezaMqsi3biwLwuHIJdK6+oqPQ1VhUzWJRtSGhF8/pk+3ZVhSIp12WQO/C2zCk4R3O+6dawKJqw81RgXB3dhQdg8jkvioMwrNuS6B38RIdhQAgJkl0AmmxqNrg6uTI039ks/5b0BnPuy+B3tlTdBTiab8WsajaYXSfzqIjEJnNF/khWOS5GHonTsQsjIs30PUW0SmkxaJqh5G9OkPpwHU/yHZ9mheGF70WQ+/kLjqKfYoaCSidRaeQFouqHXzcnTC4G4fzkm1bnReO13wWQa/kLRkWFzNWdAKpsaja6fZ+IaIjEJndJxe74E2/hdA7uoiOYj8clED0KNEppMaiaqc7+4fCzYmj/8j2/etCVywLWAi9I09FWUTkTYCbr+gUUmNRtZO3qxOPqshuvJfTHR8GLITewUl0FNvX6w7RCaTHojLAPYM5BxfZj7dzeuA/nRZA76AUHcV2KV2BfpNFp5Aei8oAg7r6IyqI95uQ/Xj9fE+s7PwP6BU87W0Wve8E3DlQqy0sKgNNHcijKrIvL5/rhU+Dn4dewZcLkxv4gOgEVoG/eQaadEM4nB35z0b2ZdG5WHwe8hzLypQ69bo8kILaxN86A/l7OGNULGeqIPvzj6x+2Bj6LPTgze8mcf1M0QmsBovKCBxUQfZqXuZ1+DLs7yyrjlK6Av2niU5hNVhURhgWFYhwP969T/bp6cwB2Bb+tOgY1q3PBMDNT3QKq8GiMoJCoeCgCrJrczNuwPbwJ0THsF5DHu7wLvbv3w9HR0ckJV1eHmTWrFlQKBQdeqxevRoAUFNTAz8/P/j7+6OmpqbZ59+0aRNGjBgBHx8feHp6Ii4uDi+99BJKS0sBAKtXr4avr2+T70lLS0N4eDgmTpyIurq6dv+sLCojTR4YAUdOVEt27OGMIdgVMVd0DOsTebNJFkhcuXIl5s6di3379iEnJwfvvfce8vPzGx8AsGrVqsa/nz9/vsnXp0yZgqSkpCafmzp1KoDLJdS3b1/ExsZi8+bN1zz3ggULMHXqVAwaNAjfffcdTpw4gbfffhtHjx7Fp59+2mzeQ4cO4ZZbbkFiYiI2btwIF5f2T9PFO/mMFOzjiqS+wfjmWL7oKETCzEm/EaujtRhx4WPRUazH0L91eBdqtRobNmzAoUOHUFBQgNWrV2PRokXw8Wm6YrOvry+Cg4Ob3Yebmxvq6uqa/XpycjJmzJgBvV6P5ORkTJ8+vfFrBw8exKuvvoply5bhiSf+PKru2rUrRo0ahfLy8mv2t3v3btx11114+OGH8c9//tPgn5dHVB3weEI0FDyoIjs3K30Yfo74q+gY1sGvq0lmSl+/fj1iYmIQExODGTNmYNWqVdDr9R3PByAzMxO//PILpkyZgilTpmD//v3Iyspq/Pq6devg6emJRx55pNnvv/p035dffomxY8diwYIFRpUUwKLqkJhgLyT1af7dCpE9mZ5+Kw5EPCg6hvwG/xVw6PjL7h9HPACQlJQElUqFXbt2dXi/wOVTimPGjGm8RpWUlISVK1c2fj09PR3du3eHk1Pb80CqVCpMnjwZzz77LObPn290JhZVBz0+kkdVRAAwNT0Bh7vMEh1DXs5ewIAZHd7NmTNncPDgQdxzzz0AAKVSialTpzYpE2NptVqsWbOmsQQBYMaMGVizZg20Wi0AQK/XQ9HOFz03NzeMGjUKy5cvR1pamtG5eI2qg3qHeGN0bGd8f7JQdBQi4SaeHY0t0VrEXWj+grpdG/wQ4Ord4d0kJydDo9EgLCys8XN6vR5OTk4oKyuDn5/xw96///575ObmNg6q+INWq8WOHTswZswY9OzZE/v27UNDQ0ObR1WOjo746quvMGnSJMTHx2P37t2IjY01OBePqEyAR1VEf7ozfQxORtwrOoZc3PyBYU92eDcajQZr167F22+/jdTU1MbH0aNHERkZiXXr1nVo/8nJybjnnnua7Ds1NRXTp09HcnIyAODee++FSqXCxx83P4Dm6sEULi4u2Lx5MwYPHoz4+HicOHHC4Fw8ojKBPqE+uK13Z/xwikdVRAAwNv0ObI/WoteF9aKjyGH4s4CrT9vbtWHbtm0oKyvDnDlzrhnhd/fddyM5ORmPPfaYUfsuKirC1q1bsWXLFvTt27fJ12bOnImxY8eiqKgIQ4YMwbx58/DMM88gNzcXEyZMQGhoKDIyMvDvf/8bw4YNazIaEACcnZ2xadMmTJkyBQkJCdi1axf69evX7mw8ojKRJ0ZGi45AJJUxGXciPeJu0THE840EBplmoElycjJuu+22a0oKACZNmoTU1FQcPnzYqH2vXbsWHh4eGDly5DVfi4+Ph5eXV+M9Um+88QY+++wzHDhwAImJiejTpw+efvppxMXFYebM5ucwdHJywoYNGzB8+HAkJCTg2LFj7c6m0JtqTCPhwTWHsDPtkugYRNJQKPTY1eO/6H7xS9FRxJm4HIibIjqFVeMRlQk9MbKn6AhEUtHrFRiVOQnnw+8UHUWM4Diu4GsCLCoT6hfug4ReQaJjEElFq3dAQuYUXAjv+I2uVmfUi+BIq45jUZnYU7f1BKcAJGpKq3dAQta9yAtLEh3FcrrHAz0SRKewCSwqE+sX7oOpg7qIjkEknQadAvHnZqAgbJToKBaguHw0RSbBojKD55Ji4O/hLDoGkXTqdA6Iz74fl0KvHVlmU/pNBkKuE53CZrCozMDX3Rnzk3qJjkEkpRqtI+LPP4Di0BGio5iHowuQ8ILoFDaFRWUmkweG44ZIruBJ1By11gG35sxBacgtoqOY3qA5gF+k6BQ2hUVlJgqFAi/f1ZeLKxK1QK1xxK0XHkJ58E2io5iOi8/lWSjIpFhUZhQb6o37b+Q7K6KWVGmUiM99GJWdh4iOYhojFwLu/qJT2BwWlZk9Paongrzav+Qykb0pa1AiPv8RVAUNFB2lY7qPMNlUSdQUi8rMvFydsGBsb9ExiKRWUu+EkYVzoep0vegoxnHxBu76iDf3mgmLygLu6h+Gm3oEiI5BJLVLdU647dLjUHfqLzqK4ZJeA3zCRaewWSwqC3nprr5wduQ/N1FrCuqcMbroCVQHtn8JCOF6Jplk5V5qGV85LSQqyBN/G9FDdAwi6eXWuiCp5CnUBPQRHaVtbn7AuPdFp7B5LCoLmpsQhf4RvqJjEEkvp8YVY8ueQZ1/jOgorbv9LcCrs+gUNo9FZUFKRwe8d09/eDg7io5CJL2salfcUTEPdX6SLp8TOx7ox4UhLYFFZWGRAR5YfKcVnNIgkkC62g13Vc1Dva9kp809OgFj3xGdwm6wqASYMjACt/cLFh2DyCqcVrljgno+Gny6iY7yp3HvAR4cyWspLCpBXpsQh1AfV9ExiKzCySoPTKp5Hg3eEsz0EncP0MsOF4EUiEUliI+7Ez6493o4OfIGQaL2OFbpial1C6DxjhAXwisUGPOGuOe3UywqgW6I9MO8RC4HQtRehys8cW/9Ami8wiz/5A5OwKTlgJuv5Z/bzrGoBHtoeHfc1pvDW4na62C5N+7TvACtZ4hln3jsW0DXYZZ9TgLAopLC25OvQ7ifm+gYRFbjlzIfPKBbCK2Hhd7kDfkbcMMsyzwXXYNFJQEfdyd8dO/1cFHyPwdRe+0p9cVDWASde6B5nyhqFJC41LzPQa3iK6MkrovwxbtT+3PyZSID7C7xw18dlkDnZqay6tQLuHsl4MCb9EViUUnk9n4h+McYLglCZIgfiv3xqHIRdG4mXrDQzR+Y9gXg6m3a/ZLBWFSSeWh4d8zkqsBEBvmuKBBPOC2GztXXNDt0cAKmfgr4S3STsR1jUUlo8bg+HAlIZKCtlzrh7y5LoHcxwRHQHe9whJ9EWFQScnBQ4INpA3BduI/oKERWZXNhEOa7L4He2dP4nQx9FLj+ftOFog5jUUnKzdkRybMGIcKfw9aJDLE+PxgveCyB3tnD8G+OHg2MfsX0oahDWFQSC/R0wapZg+Hj5iQ6CpFVWZcfiiWeS6B3cm//N3XqDUxKBhz4sigb/heRXFSQJ5bfPxDOvMeKyCBr8sLwivdi6JXtOCvhGQzcyxF+suKrnxUY3M0fb02+jvdYERkoOTcCb/otgl7ZykoFnp2BWdsAv64Wy0WGYVFZiTuvC8XLd/VlWREZ6F8XIvGO/0LoHV2u/aJHJ2DmViAw2vLBqN1YVFZkxtBIvDEpDg4sKyKDfJDTDe8HLITe0fnPT7oHXi6pTjHiglG7KPR6vV50CDLM16m5eGbDUWh0/E9HZIh5ken4W9HLULh4Xy6p4L6iI1E7sKis1PYT+Xj881TUa3WioxBZlZd7nsN9Y4YDIXGio1A78dSflUrqG4J/33c9RwMSGSDQ0wWDb5/JkrIyPKKycvvSi/HQ2hTUNGhFRyGSWqiPK9Y9NBTdAo24EZiEYlHZgIPnSjF79SGo6jSioxBJKTLAHeseHIJwPwNuACZpsKhsxJGcMsxceRCVtSwroitFB3li3YNDEOTdyr1UJDUWlQ05kVuB+1ceRKm6XnQUIikM7e6Pf02/AX4ezm1vTNJiUdmY8yVqPLQ2BWcLVaKjEAk1fUgXLLmzD5wcOeDI2rGobJC6ToOnN6Ti+5OFoqMQWZzSQYFF42Jx/41dRUchE2FR2Si9Xo9lO9Px/u508L8w2Qtfdyd8fO/1uCkqUHQUMiEWlY3bfqIAz2xIhbqew9fJtkUFeSJ55kBEBnD4ua1hUdmBMwVVeGhtCnJKq0VHITKL+JhOeH/aAHi5cu02W8SishPl1fV47LMj2JdRLDoKkUn9ZXh3zE/qBQfO1myzWFR2RKvTY+k3aVj58znRUYg6zFnpgNcm9MOkG8JFRyEzY1HZof/+dhELvjyOOg0ntCXr1L2TB5ZN7Y+4cF/RUcgCWFR26kxBFZ744ghOF1SJjkLUbgoFMPPGrpg/phdcnRxFxyELYVHZsXqNDm/tOIMVe7PApa1IdiE+rvjn3ddhWDSHntsbFhXhl8wS/H3jUeSW14iOQtSs8f1D8eJdfeHjxlF99ohFRQCAytoGLPn6JDYfyRUdhaiRr7sTlo7vh7FxIaKjkEAsKmpi9+lC/GPzCRRU1oqOQnZuREwnvDkpjrOeE4uKrlVZ24Cl29KwPuWC6Chkh9ydHfGP23tjxtBI0VFIEiwqatHe9CLM33Sc167IYm7t2Qkv3tkHXbkKL12BRUWtqq7X4D//y8Ine7K43D2ZTVSQJxaM7Y34mCDRUUhCLCpql8LKWrz1/RlsOnyRQ9nJZHzdnfDEyGjcNzQSSq4bRS1gUZFBTuZVYOk3adifWSI6ClkxpYMCM4ZG4snbouHrztV3qXUsKjLKzlOFePW7NGQVqUVHISszIqYTXhgbi6ggT9FRyEqwqMhoGq0Onx3MwbKd6ShV14uOQ5LjdSgyFouKOqyytgEf7c7Aqv3ZqOdEt3SVQE8XPBbfAzN4HYqMxKIik7lYVo3le7Kw8beLqOaKwnYvzNcNf721O6YMjOAEstQhLCoyufLqeqw7kIPV+7NRVFUnOg5ZWPdADzw8ogcmDAiDE4+gyARYVGQ2dRotvj6ShxX7snC2UCU6DpnZdeE+eGh4d9zeN4Sr7ZJJsajI7PR6PX46W4Tle7I4rN3GOCiAUbGd8eAt3TGoq7/oOGSjWFRkUSdyK7Bibxa2HcuHhncOWy0PZ0dMHhiB2Td3Q5cAd9FxyMaxqEiIvPIafHYgB1+l5uJiGecStAYOCuCmHoGYMCAMSX2D4eGiFB2J7ASLioT77Xwpvk7NwzfH8lHC+7Gk0zvEGxMGhOKu/mHozCU3SAAWFUlDo9Vhb0YxtqTmYcfJAqg5xF2YYG9X3NU/FBOuD0OvYG/RccjOsahISjX1WvyQVogtqbn439kiNGj5a2puni5KJPYJxsTrw3Bj9wCO3CNpsKhIeuXV9fj2eAG+O5GPg+dKUcfZL0wmwt8Nw6ICcUt0J8THBMHNmTfmknxYVGRVahu0OHCuFHvPFmFvejHOFFaJjmRVfN2dcGP3AAyLDsSwqEBEBnCBQpIfi4qsWmFlLX7OKMaBrFIczC7FuWLO5n4lZ6UDBkb64eaoQNwSHYi+oT48pUdWh0VFNuVSZS0OnCvFgXMlSMkuQ2aRyq6ubwV6uqB3iBdiQ71xc49ADO7mz3n2yOqxqMimNWh1yC5W40xhFc4WqpBeWIWzhVU4X1Jt1Tccuzo5oGdnL/QK9kJMsDd6BV/+OMDTRXQ0IpNjUZFdqtfokFmkwtnCKqQX/v7nJRUullVLdQTm4+aEYG9XdA10R0ywN3oHeyEm2AtdAzx4Co/sBouK6CqVtQ0oVdWjRF2PUnU9StV1lz9WXf77n5+vR1l1PbQ6PRQKQAHF738CCoUCCgC48u+/f6x0dICfuxP83J0vPzwuf+zv4YxOXi4I9nZFsI8rOnu78rQdEVhUREQkOS4WQ0REUmNRERGR1FhUREQkNRYVERFJjUVFRERSY1EREZHUWFRERCQ1FhUREUmNRUVERFJjURERkdRYVEREJDUWFRERSY1FRUREUmNRERGR1FhUREQkNRYVERFJjUVFRERSY1EREZHUWFRERCQ1FhUREUmNRUVERFJjURERkdRYVEREJDUWFRERSY1FRUREUmNRERGR1FhUREQkNRYVERFJjUVFRERSY1EREZHUWFRERCQ1FhUREUmNRUVERFJjURERkdRYVEREJDUWFRERSY1FRUREUmNRERGR1FhUREQkNRYVERFJjUVFRERSY1EREZHUWFRERCQ1FhUREUmNRUVERFJjURERkdT+HySpI40J8S/aAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_path = \"C:/Users/katsa/OneDrive/Jupyter_files/shallow_models_cic_nb15/nb_12_feat_train_dataset.csv\"\n",
    "CICDataset = MyDataset(sample_path, selected_features_total)\n",
    "batch_size = 64\n",
    "dataloader = DataLoader(CICDataset, batch_size=batch_size, \n",
    "                          shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e7f2c41-7fd2-4c2c-9558-911d2cf1f8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator model\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim + 1, 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(8, 16),\n",
    "            nn.BatchNorm1d(16, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.BatchNorm1d(8, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(8, data_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z, label):\n",
    "        label = torch.full((batch_size, 1), label)\n",
    "        z = torch.cat((noise, label), 1)\n",
    "        data = self.model(z)\n",
    "        return data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a1b526a-6105-48fe-883f-3113f2480a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the discriminator model\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(data_dim, 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(4, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        validity = self.model(data)\n",
    "        return validity\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80938735-654d-4141-884f-d09bf28bf403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "# Definition of hyperparameters\n",
    "\n",
    "data_dim=len(selected_features_total)\n",
    "print(data_dim)\n",
    "latent_dim = 4\n",
    "lr = 0.0002\n",
    "adversarial_loss = nn.BCELoss()\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c663729-ff33-4ea9-a470-0e1cf2c7b886",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(batch_size, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10da37bd-24f5-4771-8a28-918cb1772d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_lst = [5, 10, 15, 20, 25, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab3315ec-ec36-40ad-adf1-4b2a582b9ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/100] [Batch 0/6350] [D loss: 2.359735] [G loss: 1.447681]\n",
      "[Epoch 0/100] [Batch 1000/6350] [D loss: 2.025642] [G loss: 1.338949]\n",
      "[Epoch 0/100] [Batch 2000/6350] [D loss: 1.719475] [G loss: 0.947216]\n",
      "[Epoch 0/100] [Batch 3000/6350] [D loss: 1.768679] [G loss: 0.996437]\n",
      "[Epoch 0/100] [Batch 4000/6350] [D loss: 1.628668] [G loss: 1.014016]\n",
      "[Epoch 0/100] [Batch 5000/6350] [D loss: 1.711826] [G loss: 0.922117]\n",
      "[Epoch 0/100] [Batch 6000/6350] [D loss: 1.760822] [G loss: 0.853376]\n",
      "Saving models...\n",
      "[Epoch 1/100] [Batch 0/6350] [D loss: 1.769934] [G loss: 0.851438]\n",
      "[Epoch 1/100] [Batch 1000/6350] [D loss: 1.725031] [G loss: 0.831326]\n",
      "[Epoch 1/100] [Batch 2000/6350] [D loss: 1.764603] [G loss: 0.835917]\n",
      "[Epoch 1/100] [Batch 3000/6350] [D loss: 1.737894] [G loss: 0.837232]\n",
      "[Epoch 1/100] [Batch 4000/6350] [D loss: 1.680562] [G loss: 0.822961]\n",
      "[Epoch 1/100] [Batch 5000/6350] [D loss: 1.746369] [G loss: 0.793813]\n",
      "[Epoch 1/100] [Batch 6000/6350] [D loss: 1.787966] [G loss: 0.799771]\n",
      "Saving models...\n",
      "[Epoch 2/100] [Batch 0/6350] [D loss: 1.806351] [G loss: 0.795390]\n",
      "[Epoch 2/100] [Batch 1000/6350] [D loss: 1.770697] [G loss: 0.794924]\n",
      "[Epoch 2/100] [Batch 2000/6350] [D loss: 1.760707] [G loss: 0.790244]\n",
      "[Epoch 2/100] [Batch 3000/6350] [D loss: 1.884428] [G loss: 0.778553]\n",
      "[Epoch 2/100] [Batch 4000/6350] [D loss: 1.802455] [G loss: 0.802823]\n",
      "[Epoch 2/100] [Batch 5000/6350] [D loss: 1.758305] [G loss: 0.775282]\n",
      "[Epoch 2/100] [Batch 6000/6350] [D loss: 1.847287] [G loss: 0.753776]\n",
      "Saving models...\n",
      "[Epoch 3/100] [Batch 0/6350] [D loss: 1.772676] [G loss: 0.763360]\n",
      "[Epoch 3/100] [Batch 1000/6350] [D loss: 1.714449] [G loss: 0.797151]\n",
      "[Epoch 3/100] [Batch 2000/6350] [D loss: 1.791872] [G loss: 0.789165]\n",
      "[Epoch 3/100] [Batch 3000/6350] [D loss: 1.798931] [G loss: 0.787747]\n",
      "[Epoch 3/100] [Batch 4000/6350] [D loss: 1.800108] [G loss: 0.787576]\n",
      "[Epoch 3/100] [Batch 5000/6350] [D loss: 1.729051] [G loss: 0.780127]\n",
      "[Epoch 3/100] [Batch 6000/6350] [D loss: 1.905629] [G loss: 0.781549]\n",
      "Saving models...\n",
      "[Epoch 4/100] [Batch 0/6350] [D loss: 1.761739] [G loss: 0.783802]\n",
      "[Epoch 4/100] [Batch 1000/6350] [D loss: 1.793750] [G loss: 0.787400]\n",
      "[Epoch 4/100] [Batch 2000/6350] [D loss: 1.769513] [G loss: 0.777704]\n",
      "[Epoch 4/100] [Batch 3000/6350] [D loss: 1.798068] [G loss: 0.781241]\n",
      "[Epoch 4/100] [Batch 4000/6350] [D loss: 1.878236] [G loss: 0.769976]\n",
      "[Epoch 4/100] [Batch 5000/6350] [D loss: 1.809998] [G loss: 0.779696]\n",
      "[Epoch 4/100] [Batch 6000/6350] [D loss: 1.757044] [G loss: 0.783709]\n",
      "Saving models...\n",
      "[Epoch 5/100] [Batch 0/6350] [D loss: 1.763759] [G loss: 0.783705]\n",
      "[Epoch 5/100] [Batch 1000/6350] [D loss: 1.784247] [G loss: 0.771426]\n",
      "[Epoch 5/100] [Batch 2000/6350] [D loss: 1.835493] [G loss: 0.780397]\n",
      "[Epoch 5/100] [Batch 3000/6350] [D loss: 1.825096] [G loss: 0.779211]\n",
      "[Epoch 5/100] [Batch 4000/6350] [D loss: 1.790247] [G loss: 0.768756]\n",
      "[Epoch 5/100] [Batch 5000/6350] [D loss: 1.778987] [G loss: 0.779796]\n",
      "[Epoch 5/100] [Batch 6000/6350] [D loss: 1.798261] [G loss: 0.777060]\n",
      "Saving models...\n",
      "[Epoch 6/100] [Batch 0/6350] [D loss: 1.775628] [G loss: 0.780714]\n",
      "[Epoch 6/100] [Batch 1000/6350] [D loss: 1.808375] [G loss: 0.775880]\n",
      "[Epoch 6/100] [Batch 2000/6350] [D loss: 1.834322] [G loss: 0.775193]\n",
      "[Epoch 6/100] [Batch 3000/6350] [D loss: 1.786119] [G loss: 0.770162]\n",
      "[Epoch 6/100] [Batch 4000/6350] [D loss: 1.778096] [G loss: 0.779193]\n",
      "[Epoch 6/100] [Batch 5000/6350] [D loss: 1.755823] [G loss: 0.773862]\n",
      "[Epoch 6/100] [Batch 6000/6350] [D loss: 1.777865] [G loss: 0.779373]\n",
      "Saving models...\n",
      "[Epoch 7/100] [Batch 0/6350] [D loss: 1.762413] [G loss: 0.776175]\n",
      "[Epoch 7/100] [Batch 1000/6350] [D loss: 1.819607] [G loss: 0.820240]\n",
      "[Epoch 7/100] [Batch 2000/6350] [D loss: 1.717541] [G loss: 0.796680]\n",
      "[Epoch 7/100] [Batch 3000/6350] [D loss: 1.820604] [G loss: 0.780484]\n",
      "[Epoch 7/100] [Batch 4000/6350] [D loss: 1.780424] [G loss: 0.781174]\n",
      "[Epoch 7/100] [Batch 5000/6350] [D loss: 1.771119] [G loss: 0.779397]\n",
      "[Epoch 7/100] [Batch 6000/6350] [D loss: 1.802099] [G loss: 0.777382]\n",
      "Saving models...\n",
      "[Epoch 8/100] [Batch 0/6350] [D loss: 1.787725] [G loss: 0.779976]\n",
      "[Epoch 8/100] [Batch 1000/6350] [D loss: 1.811780] [G loss: 0.775645]\n",
      "[Epoch 8/100] [Batch 2000/6350] [D loss: 1.782494] [G loss: 0.780188]\n",
      "[Epoch 8/100] [Batch 3000/6350] [D loss: 1.744181] [G loss: 0.786207]\n",
      "[Epoch 8/100] [Batch 4000/6350] [D loss: 1.733488] [G loss: 0.783227]\n",
      "[Epoch 8/100] [Batch 5000/6350] [D loss: 1.771628] [G loss: 0.774229]\n",
      "[Epoch 8/100] [Batch 6000/6350] [D loss: 1.756805] [G loss: 0.781922]\n",
      "Saving models...\n",
      "[Epoch 9/100] [Batch 0/6350] [D loss: 1.755240] [G loss: 0.785514]\n",
      "[Epoch 9/100] [Batch 1000/6350] [D loss: 2.254603] [G loss: 0.720428]\n",
      "[Epoch 9/100] [Batch 2000/6350] [D loss: 1.715316] [G loss: 0.808627]\n",
      "[Epoch 9/100] [Batch 3000/6350] [D loss: 1.743439] [G loss: 0.790558]\n",
      "[Epoch 9/100] [Batch 4000/6350] [D loss: 1.769051] [G loss: 0.821882]\n",
      "[Epoch 9/100] [Batch 5000/6350] [D loss: 1.748554] [G loss: 0.788058]\n",
      "[Epoch 9/100] [Batch 6000/6350] [D loss: 1.768654] [G loss: 0.759709]\n",
      "Saving models...\n",
      "[Epoch 10/100] [Batch 0/6350] [D loss: 1.847227] [G loss: 0.751204]\n",
      "[Epoch 10/100] [Batch 1000/6350] [D loss: 1.731390] [G loss: 0.774734]\n",
      "[Epoch 10/100] [Batch 2000/6350] [D loss: 1.786267] [G loss: 0.778793]\n",
      "[Epoch 10/100] [Batch 3000/6350] [D loss: 1.806703] [G loss: 0.791148]\n",
      "[Epoch 10/100] [Batch 4000/6350] [D loss: 1.796445] [G loss: 0.774859]\n",
      "[Epoch 10/100] [Batch 5000/6350] [D loss: 1.797156] [G loss: 0.777631]\n",
      "[Epoch 10/100] [Batch 6000/6350] [D loss: 1.723148] [G loss: 0.799938]\n",
      "Saving models...\n",
      "[Epoch 11/100] [Batch 0/6350] [D loss: 1.729216] [G loss: 0.795776]\n",
      "[Epoch 11/100] [Batch 1000/6350] [D loss: 1.724987] [G loss: 0.786453]\n",
      "[Epoch 11/100] [Batch 2000/6350] [D loss: 1.831670] [G loss: 0.745780]\n",
      "[Epoch 11/100] [Batch 3000/6350] [D loss: 1.739334] [G loss: 0.770727]\n",
      "[Epoch 11/100] [Batch 4000/6350] [D loss: 1.761753] [G loss: 0.753005]\n",
      "[Epoch 11/100] [Batch 5000/6350] [D loss: 1.780952] [G loss: 0.748516]\n",
      "[Epoch 11/100] [Batch 6000/6350] [D loss: 1.774923] [G loss: 0.756634]\n",
      "Saving models...\n",
      "[Epoch 12/100] [Batch 0/6350] [D loss: 1.752700] [G loss: 0.749072]\n",
      "[Epoch 12/100] [Batch 1000/6350] [D loss: 1.756830] [G loss: 0.754912]\n",
      "[Epoch 12/100] [Batch 2000/6350] [D loss: 1.763703] [G loss: 0.755885]\n",
      "[Epoch 12/100] [Batch 3000/6350] [D loss: 1.739592] [G loss: 0.763846]\n",
      "[Epoch 12/100] [Batch 4000/6350] [D loss: 1.804546] [G loss: 0.755271]\n",
      "[Epoch 12/100] [Batch 5000/6350] [D loss: 1.726495] [G loss: 0.757239]\n",
      "[Epoch 12/100] [Batch 6000/6350] [D loss: 1.803232] [G loss: 0.750975]\n",
      "Saving models...\n",
      "[Epoch 13/100] [Batch 0/6350] [D loss: 1.785656] [G loss: 0.756628]\n",
      "[Epoch 13/100] [Batch 1000/6350] [D loss: 1.739631] [G loss: 0.760456]\n",
      "[Epoch 13/100] [Batch 2000/6350] [D loss: 1.797383] [G loss: 0.756540]\n",
      "[Epoch 13/100] [Batch 3000/6350] [D loss: 1.715241] [G loss: 0.756185]\n",
      "[Epoch 13/100] [Batch 4000/6350] [D loss: 1.718611] [G loss: 0.765699]\n",
      "[Epoch 13/100] [Batch 5000/6350] [D loss: 1.744325] [G loss: 0.759530]\n",
      "[Epoch 13/100] [Batch 6000/6350] [D loss: 1.729166] [G loss: 0.763441]\n",
      "Saving models...\n",
      "[Epoch 14/100] [Batch 0/6350] [D loss: 1.771860] [G loss: 0.762346]\n",
      "[Epoch 14/100] [Batch 1000/6350] [D loss: 1.749441] [G loss: 0.761525]\n",
      "[Epoch 14/100] [Batch 2000/6350] [D loss: 1.728214] [G loss: 0.755076]\n",
      "[Epoch 14/100] [Batch 3000/6350] [D loss: 1.758208] [G loss: 0.763263]\n",
      "[Epoch 14/100] [Batch 4000/6350] [D loss: 1.743530] [G loss: 0.759472]\n",
      "[Epoch 14/100] [Batch 5000/6350] [D loss: 1.744503] [G loss: 0.763026]\n",
      "[Epoch 14/100] [Batch 6000/6350] [D loss: 1.740147] [G loss: 0.759967]\n",
      "Saving models...\n",
      "[Epoch 15/100] [Batch 0/6350] [D loss: 1.797800] [G loss: 0.695530]\n",
      "[Epoch 15/100] [Batch 1000/6350] [D loss: 1.742043] [G loss: 0.731757]\n",
      "[Epoch 15/100] [Batch 2000/6350] [D loss: 1.742487] [G loss: 0.742450]\n",
      "[Epoch 15/100] [Batch 3000/6350] [D loss: 1.830972] [G loss: 0.736348]\n",
      "[Epoch 15/100] [Batch 4000/6350] [D loss: 1.875950] [G loss: 0.689725]\n",
      "[Epoch 15/100] [Batch 5000/6350] [D loss: 1.964423] [G loss: 0.631357]\n",
      "[Epoch 15/100] [Batch 6000/6350] [D loss: 1.747868] [G loss: 0.666381]\n",
      "Saving models...\n",
      "[Epoch 16/100] [Batch 0/6350] [D loss: 1.779381] [G loss: 0.865808]\n",
      "[Epoch 16/100] [Batch 1000/6350] [D loss: 1.714517] [G loss: 0.879749]\n",
      "[Epoch 16/100] [Batch 2000/6350] [D loss: 1.683467] [G loss: 0.814676]\n",
      "[Epoch 16/100] [Batch 3000/6350] [D loss: 1.770824] [G loss: 0.788642]\n",
      "[Epoch 16/100] [Batch 4000/6350] [D loss: 1.690849] [G loss: 0.776809]\n",
      "[Epoch 16/100] [Batch 5000/6350] [D loss: 1.723910] [G loss: 0.788840]\n",
      "[Epoch 16/100] [Batch 6000/6350] [D loss: 1.677204] [G loss: 0.788066]\n",
      "Saving models...\n",
      "[Epoch 17/100] [Batch 0/6350] [D loss: 1.750136] [G loss: 0.818767]\n",
      "[Epoch 17/100] [Batch 1000/6350] [D loss: 1.707742] [G loss: 0.789382]\n",
      "[Epoch 17/100] [Batch 2000/6350] [D loss: 1.735183] [G loss: 0.780206]\n",
      "[Epoch 17/100] [Batch 3000/6350] [D loss: 1.672705] [G loss: 0.813799]\n",
      "[Epoch 17/100] [Batch 4000/6350] [D loss: 1.722682] [G loss: 0.807017]\n",
      "[Epoch 17/100] [Batch 5000/6350] [D loss: 1.686654] [G loss: 0.764535]\n",
      "[Epoch 17/100] [Batch 6000/6350] [D loss: 1.730763] [G loss: 0.764598]\n",
      "Saving models...\n",
      "[Epoch 18/100] [Batch 0/6350] [D loss: 1.706214] [G loss: 0.768813]\n",
      "[Epoch 18/100] [Batch 1000/6350] [D loss: 1.752886] [G loss: 0.765339]\n",
      "[Epoch 18/100] [Batch 2000/6350] [D loss: 1.696957] [G loss: 0.753041]\n",
      "[Epoch 18/100] [Batch 3000/6350] [D loss: 1.701662] [G loss: 0.781871]\n",
      "[Epoch 18/100] [Batch 4000/6350] [D loss: 1.737824] [G loss: 0.757180]\n",
      "[Epoch 18/100] [Batch 5000/6350] [D loss: 1.734295] [G loss: 0.786460]\n",
      "[Epoch 18/100] [Batch 6000/6350] [D loss: 1.690822] [G loss: 0.771475]\n",
      "Saving models...\n",
      "[Epoch 19/100] [Batch 0/6350] [D loss: 1.692656] [G loss: 0.768153]\n",
      "[Epoch 19/100] [Batch 1000/6350] [D loss: 1.695676] [G loss: 0.755666]\n",
      "[Epoch 19/100] [Batch 2000/6350] [D loss: 1.710354] [G loss: 0.774642]\n",
      "[Epoch 19/100] [Batch 3000/6350] [D loss: 1.756011] [G loss: 0.754990]\n",
      "[Epoch 19/100] [Batch 4000/6350] [D loss: 1.716164] [G loss: 0.779211]\n",
      "[Epoch 19/100] [Batch 5000/6350] [D loss: 1.723011] [G loss: 0.736598]\n",
      "[Epoch 19/100] [Batch 6000/6350] [D loss: 1.684875] [G loss: 0.761364]\n",
      "Saving models...\n",
      "[Epoch 20/100] [Batch 0/6350] [D loss: 1.730345] [G loss: 0.770843]\n",
      "[Epoch 20/100] [Batch 1000/6350] [D loss: 1.705528] [G loss: 0.789085]\n",
      "[Epoch 20/100] [Batch 2000/6350] [D loss: 1.781471] [G loss: 0.778817]\n",
      "[Epoch 20/100] [Batch 3000/6350] [D loss: 2.350007] [G loss: 0.768527]\n",
      "[Epoch 20/100] [Batch 4000/6350] [D loss: 2.977857] [G loss: 0.723519]\n",
      "[Epoch 20/100] [Batch 5000/6350] [D loss: 1.781053] [G loss: 0.784273]\n",
      "[Epoch 20/100] [Batch 6000/6350] [D loss: 1.875385] [G loss: 0.681297]\n",
      "Saving models...\n",
      "[Epoch 21/100] [Batch 0/6350] [D loss: 1.837925] [G loss: 0.731400]\n",
      "[Epoch 21/100] [Batch 1000/6350] [D loss: 1.755675] [G loss: 0.762327]\n",
      "[Epoch 21/100] [Batch 2000/6350] [D loss: 1.730724] [G loss: 0.770538]\n",
      "[Epoch 21/100] [Batch 3000/6350] [D loss: 1.689860] [G loss: 0.801399]\n",
      "[Epoch 21/100] [Batch 4000/6350] [D loss: 1.689446] [G loss: 0.782860]\n",
      "[Epoch 21/100] [Batch 5000/6350] [D loss: 1.694772] [G loss: 0.794220]\n",
      "[Epoch 21/100] [Batch 6000/6350] [D loss: 1.667827] [G loss: 0.837109]\n",
      "Saving models...\n",
      "[Epoch 22/100] [Batch 0/6350] [D loss: 1.714586] [G loss: 0.746563]\n",
      "[Epoch 22/100] [Batch 1000/6350] [D loss: 1.693252] [G loss: 0.760099]\n",
      "[Epoch 22/100] [Batch 2000/6350] [D loss: 1.697605] [G loss: 0.802907]\n",
      "[Epoch 22/100] [Batch 3000/6350] [D loss: 1.698202] [G loss: 0.778255]\n",
      "[Epoch 22/100] [Batch 4000/6350] [D loss: 1.733696] [G loss: 0.769394]\n",
      "[Epoch 22/100] [Batch 5000/6350] [D loss: 1.711011] [G loss: 0.779456]\n",
      "[Epoch 22/100] [Batch 6000/6350] [D loss: 1.728143] [G loss: 0.775302]\n",
      "Saving models...\n",
      "[Epoch 23/100] [Batch 0/6350] [D loss: 1.720129] [G loss: 0.767341]\n",
      "[Epoch 23/100] [Batch 1000/6350] [D loss: 1.696025] [G loss: 0.773925]\n",
      "[Epoch 23/100] [Batch 2000/6350] [D loss: 1.717083] [G loss: 0.746598]\n",
      "[Epoch 23/100] [Batch 3000/6350] [D loss: 1.720892] [G loss: 0.761711]\n",
      "[Epoch 23/100] [Batch 4000/6350] [D loss: 1.726882] [G loss: 0.771042]\n",
      "[Epoch 23/100] [Batch 5000/6350] [D loss: 1.836498] [G loss: 0.756370]\n",
      "[Epoch 23/100] [Batch 6000/6350] [D loss: 1.679608] [G loss: 0.765541]\n",
      "Saving models...\n",
      "[Epoch 24/100] [Batch 0/6350] [D loss: 1.687091] [G loss: 0.779878]\n",
      "[Epoch 24/100] [Batch 1000/6350] [D loss: 1.675184] [G loss: 0.753958]\n",
      "[Epoch 24/100] [Batch 2000/6350] [D loss: 1.698156] [G loss: 0.781990]\n",
      "[Epoch 24/100] [Batch 3000/6350] [D loss: 1.687265] [G loss: 0.778565]\n",
      "[Epoch 24/100] [Batch 4000/6350] [D loss: 1.685415] [G loss: 0.756666]\n",
      "[Epoch 24/100] [Batch 5000/6350] [D loss: 1.657164] [G loss: 0.749402]\n",
      "[Epoch 24/100] [Batch 6000/6350] [D loss: 1.690512] [G loss: 0.755210]\n",
      "Saving models...\n",
      "[Epoch 25/100] [Batch 0/6350] [D loss: 1.702234] [G loss: 0.756056]\n",
      "[Epoch 25/100] [Batch 1000/6350] [D loss: 1.674220] [G loss: 0.760127]\n",
      "[Epoch 25/100] [Batch 2000/6350] [D loss: 1.716278] [G loss: 0.783039]\n",
      "[Epoch 25/100] [Batch 3000/6350] [D loss: 1.779932] [G loss: 0.719998]\n",
      "[Epoch 25/100] [Batch 4000/6350] [D loss: 1.782876] [G loss: 0.741749]\n",
      "[Epoch 25/100] [Batch 5000/6350] [D loss: 1.719489] [G loss: 0.708442]\n",
      "[Epoch 25/100] [Batch 6000/6350] [D loss: 1.723525] [G loss: 0.722708]\n",
      "Saving models...\n",
      "[Epoch 26/100] [Batch 0/6350] [D loss: 1.695079] [G loss: 0.735151]\n",
      "[Epoch 26/100] [Batch 1000/6350] [D loss: 1.708765] [G loss: 0.781075]\n",
      "[Epoch 26/100] [Batch 2000/6350] [D loss: 1.727838] [G loss: 0.741597]\n",
      "[Epoch 26/100] [Batch 3000/6350] [D loss: 1.706998] [G loss: 0.747955]\n",
      "[Epoch 26/100] [Batch 4000/6350] [D loss: 1.778478] [G loss: 0.796082]\n",
      "[Epoch 26/100] [Batch 5000/6350] [D loss: 1.713469] [G loss: 0.783510]\n",
      "[Epoch 26/100] [Batch 6000/6350] [D loss: 1.782576] [G loss: 0.762219]\n",
      "Saving models...\n",
      "[Epoch 27/100] [Batch 0/6350] [D loss: 1.687552] [G loss: 0.767906]\n",
      "[Epoch 27/100] [Batch 1000/6350] [D loss: 1.685817] [G loss: 0.823253]\n",
      "[Epoch 27/100] [Batch 2000/6350] [D loss: 1.673779] [G loss: 0.766323]\n",
      "[Epoch 27/100] [Batch 3000/6350] [D loss: 1.655640] [G loss: 0.782678]\n",
      "[Epoch 27/100] [Batch 4000/6350] [D loss: 1.655959] [G loss: 0.760910]\n",
      "[Epoch 27/100] [Batch 5000/6350] [D loss: 1.661151] [G loss: 0.777701]\n",
      "[Epoch 27/100] [Batch 6000/6350] [D loss: 1.642369] [G loss: 0.790909]\n",
      "Saving models...\n",
      "[Epoch 28/100] [Batch 0/6350] [D loss: 1.658665] [G loss: 0.810604]\n",
      "[Epoch 28/100] [Batch 1000/6350] [D loss: 1.665903] [G loss: 0.775215]\n",
      "[Epoch 28/100] [Batch 2000/6350] [D loss: 1.686054] [G loss: 0.791084]\n",
      "[Epoch 28/100] [Batch 3000/6350] [D loss: 1.696589] [G loss: 0.769348]\n",
      "[Epoch 28/100] [Batch 4000/6350] [D loss: 1.727989] [G loss: 0.692855]\n",
      "[Epoch 28/100] [Batch 5000/6350] [D loss: 1.704983] [G loss: 0.742282]\n",
      "[Epoch 28/100] [Batch 6000/6350] [D loss: 1.650806] [G loss: 0.792486]\n",
      "Saving models...\n",
      "[Epoch 29/100] [Batch 0/6350] [D loss: 1.697398] [G loss: 0.714503]\n",
      "[Epoch 29/100] [Batch 1000/6350] [D loss: 1.655151] [G loss: 0.726678]\n",
      "[Epoch 29/100] [Batch 2000/6350] [D loss: 1.712621] [G loss: 0.706960]\n",
      "[Epoch 29/100] [Batch 3000/6350] [D loss: 1.715823] [G loss: 0.815822]\n",
      "[Epoch 29/100] [Batch 4000/6350] [D loss: 1.705429] [G loss: 0.732418]\n",
      "[Epoch 29/100] [Batch 5000/6350] [D loss: 1.677419] [G loss: 0.685794]\n",
      "[Epoch 29/100] [Batch 6000/6350] [D loss: 1.735907] [G loss: 0.760985]\n",
      "Saving models...\n",
      "[Epoch 30/100] [Batch 0/6350] [D loss: 1.669856] [G loss: 0.773840]\n",
      "[Epoch 30/100] [Batch 1000/6350] [D loss: 1.770440] [G loss: 0.837060]\n",
      "[Epoch 30/100] [Batch 2000/6350] [D loss: 1.755785] [G loss: 0.739108]\n",
      "[Epoch 30/100] [Batch 3000/6350] [D loss: 1.659167] [G loss: 0.757409]\n",
      "[Epoch 30/100] [Batch 4000/6350] [D loss: 1.673982] [G loss: 0.788812]\n",
      "[Epoch 30/100] [Batch 5000/6350] [D loss: 1.640486] [G loss: 0.802574]\n",
      "[Epoch 30/100] [Batch 6000/6350] [D loss: 1.641810] [G loss: 0.797096]\n",
      "Saving models...\n",
      "[Epoch 31/100] [Batch 0/6350] [D loss: 1.663868] [G loss: 0.798650]\n",
      "[Epoch 31/100] [Batch 1000/6350] [D loss: 1.637399] [G loss: 0.797292]\n",
      "[Epoch 31/100] [Batch 2000/6350] [D loss: 1.634115] [G loss: 0.795547]\n",
      "[Epoch 31/100] [Batch 3000/6350] [D loss: 1.638814] [G loss: 0.837961]\n",
      "[Epoch 31/100] [Batch 4000/6350] [D loss: 1.609683] [G loss: 0.789637]\n",
      "[Epoch 31/100] [Batch 5000/6350] [D loss: 1.627103] [G loss: 0.823460]\n",
      "[Epoch 31/100] [Batch 6000/6350] [D loss: 1.689310] [G loss: 0.811714]\n",
      "Saving models...\n",
      "[Epoch 32/100] [Batch 0/6350] [D loss: 1.638511] [G loss: 0.823797]\n",
      "[Epoch 32/100] [Batch 1000/6350] [D loss: 1.593211] [G loss: 0.851279]\n",
      "[Epoch 32/100] [Batch 2000/6350] [D loss: 1.594148] [G loss: 0.801561]\n",
      "[Epoch 32/100] [Batch 3000/6350] [D loss: 1.647102] [G loss: 0.811059]\n",
      "[Epoch 32/100] [Batch 4000/6350] [D loss: 1.614975] [G loss: 0.800280]\n",
      "[Epoch 32/100] [Batch 5000/6350] [D loss: 1.666231] [G loss: 0.779889]\n",
      "[Epoch 32/100] [Batch 6000/6350] [D loss: 1.613662] [G loss: 0.856863]\n",
      "Saving models...\n",
      "[Epoch 33/100] [Batch 0/6350] [D loss: 1.697582] [G loss: 0.841666]\n",
      "[Epoch 33/100] [Batch 1000/6350] [D loss: 1.583903] [G loss: 0.822407]\n",
      "[Epoch 33/100] [Batch 2000/6350] [D loss: 1.560137] [G loss: 0.850519]\n",
      "[Epoch 33/100] [Batch 3000/6350] [D loss: 1.649785] [G loss: 0.818811]\n",
      "[Epoch 33/100] [Batch 4000/6350] [D loss: 1.584062] [G loss: 0.834539]\n",
      "[Epoch 33/100] [Batch 5000/6350] [D loss: 1.601702] [G loss: 0.833074]\n",
      "[Epoch 33/100] [Batch 6000/6350] [D loss: 1.610273] [G loss: 0.844017]\n",
      "Saving models...\n",
      "[Epoch 34/100] [Batch 0/6350] [D loss: 1.692324] [G loss: 0.866022]\n",
      "[Epoch 34/100] [Batch 1000/6350] [D loss: 1.529563] [G loss: 0.861168]\n",
      "[Epoch 34/100] [Batch 2000/6350] [D loss: 1.549991] [G loss: 0.831770]\n",
      "[Epoch 34/100] [Batch 3000/6350] [D loss: 1.613652] [G loss: 0.832142]\n",
      "[Epoch 34/100] [Batch 4000/6350] [D loss: 1.584785] [G loss: 0.804164]\n",
      "[Epoch 34/100] [Batch 5000/6350] [D loss: 1.760112] [G loss: 0.753100]\n",
      "[Epoch 34/100] [Batch 6000/6350] [D loss: 1.664466] [G loss: 0.877736]\n",
      "Saving models...\n",
      "[Epoch 35/100] [Batch 0/6350] [D loss: 1.647725] [G loss: 0.859932]\n",
      "[Epoch 35/100] [Batch 1000/6350] [D loss: 1.605771] [G loss: 0.825468]\n",
      "[Epoch 35/100] [Batch 2000/6350] [D loss: 1.628341] [G loss: 0.806168]\n",
      "[Epoch 35/100] [Batch 3000/6350] [D loss: 1.595989] [G loss: 0.852046]\n",
      "[Epoch 35/100] [Batch 4000/6350] [D loss: 1.654845] [G loss: 0.862551]\n",
      "[Epoch 35/100] [Batch 5000/6350] [D loss: 1.613517] [G loss: 0.837258]\n",
      "[Epoch 35/100] [Batch 6000/6350] [D loss: 1.589401] [G loss: 0.866969]\n",
      "Saving models...\n",
      "[Epoch 36/100] [Batch 0/6350] [D loss: 1.585613] [G loss: 0.858466]\n",
      "[Epoch 36/100] [Batch 1000/6350] [D loss: 1.605930] [G loss: 0.843798]\n",
      "[Epoch 36/100] [Batch 2000/6350] [D loss: 1.705110] [G loss: 0.894659]\n",
      "[Epoch 36/100] [Batch 3000/6350] [D loss: 1.618843] [G loss: 0.856287]\n",
      "[Epoch 36/100] [Batch 4000/6350] [D loss: 1.580690] [G loss: 0.828004]\n",
      "[Epoch 36/100] [Batch 5000/6350] [D loss: 1.614787] [G loss: 0.852604]\n",
      "[Epoch 36/100] [Batch 6000/6350] [D loss: 1.620434] [G loss: 0.856463]\n",
      "Saving models...\n",
      "[Epoch 37/100] [Batch 0/6350] [D loss: 1.588274] [G loss: 0.864354]\n",
      "[Epoch 37/100] [Batch 1000/6350] [D loss: 1.575370] [G loss: 0.818686]\n",
      "[Epoch 37/100] [Batch 2000/6350] [D loss: 1.707134] [G loss: 0.840161]\n",
      "[Epoch 37/100] [Batch 3000/6350] [D loss: 1.561991] [G loss: 0.821059]\n",
      "[Epoch 37/100] [Batch 4000/6350] [D loss: 1.819187] [G loss: 0.742494]\n",
      "[Epoch 37/100] [Batch 5000/6350] [D loss: 1.672290] [G loss: 0.825306]\n",
      "[Epoch 37/100] [Batch 6000/6350] [D loss: 1.693733] [G loss: 0.837935]\n",
      "Saving models...\n",
      "[Epoch 38/100] [Batch 0/6350] [D loss: 1.640527] [G loss: 0.929844]\n",
      "[Epoch 38/100] [Batch 1000/6350] [D loss: 1.640076] [G loss: 0.939860]\n",
      "[Epoch 38/100] [Batch 2000/6350] [D loss: 1.670567] [G loss: 0.819775]\n",
      "[Epoch 38/100] [Batch 3000/6350] [D loss: 1.628982] [G loss: 0.813907]\n",
      "[Epoch 38/100] [Batch 4000/6350] [D loss: 1.602707] [G loss: 0.866669]\n",
      "[Epoch 38/100] [Batch 5000/6350] [D loss: 1.600599] [G loss: 0.843606]\n",
      "[Epoch 38/100] [Batch 6000/6350] [D loss: 1.624626] [G loss: 0.769956]\n",
      "Saving models...\n",
      "[Epoch 39/100] [Batch 0/6350] [D loss: 1.611669] [G loss: 0.898484]\n",
      "[Epoch 39/100] [Batch 1000/6350] [D loss: 1.623583] [G loss: 0.783768]\n",
      "[Epoch 39/100] [Batch 2000/6350] [D loss: 1.788378] [G loss: 0.733420]\n",
      "[Epoch 39/100] [Batch 3000/6350] [D loss: 1.692026] [G loss: 0.796545]\n",
      "[Epoch 39/100] [Batch 4000/6350] [D loss: 1.599308] [G loss: 0.863607]\n",
      "[Epoch 39/100] [Batch 5000/6350] [D loss: 1.721775] [G loss: 0.852462]\n",
      "[Epoch 39/100] [Batch 6000/6350] [D loss: 1.723806] [G loss: 0.841318]\n",
      "Saving models...\n",
      "[Epoch 40/100] [Batch 0/6350] [D loss: 1.612689] [G loss: 0.852684]\n",
      "[Epoch 40/100] [Batch 1000/6350] [D loss: 1.613790] [G loss: 0.856688]\n",
      "[Epoch 40/100] [Batch 2000/6350] [D loss: 1.622682] [G loss: 0.865066]\n",
      "[Epoch 40/100] [Batch 3000/6350] [D loss: 1.657220] [G loss: 0.827209]\n",
      "[Epoch 40/100] [Batch 4000/6350] [D loss: 1.683551] [G loss: 0.786599]\n",
      "[Epoch 40/100] [Batch 5000/6350] [D loss: 1.647068] [G loss: 0.790442]\n",
      "[Epoch 40/100] [Batch 6000/6350] [D loss: 1.655988] [G loss: 0.802335]\n",
      "Saving models...\n",
      "[Epoch 41/100] [Batch 0/6350] [D loss: 1.697775] [G loss: 0.791492]\n",
      "[Epoch 41/100] [Batch 1000/6350] [D loss: 1.629797] [G loss: 0.780486]\n",
      "[Epoch 41/100] [Batch 2000/6350] [D loss: 1.652915] [G loss: 0.842209]\n",
      "[Epoch 41/100] [Batch 3000/6350] [D loss: 1.568514] [G loss: 0.804822]\n",
      "[Epoch 41/100] [Batch 4000/6350] [D loss: 1.624900] [G loss: 0.828510]\n",
      "[Epoch 41/100] [Batch 5000/6350] [D loss: 1.611482] [G loss: 0.777605]\n",
      "[Epoch 41/100] [Batch 6000/6350] [D loss: 1.631459] [G loss: 0.798122]\n",
      "Saving models...\n",
      "[Epoch 42/100] [Batch 0/6350] [D loss: 1.622154] [G loss: 0.816521]\n",
      "[Epoch 42/100] [Batch 1000/6350] [D loss: 1.659056] [G loss: 0.734565]\n",
      "[Epoch 42/100] [Batch 2000/6350] [D loss: 1.686640] [G loss: 0.774981]\n",
      "[Epoch 42/100] [Batch 3000/6350] [D loss: 1.686572] [G loss: 0.788749]\n",
      "[Epoch 42/100] [Batch 4000/6350] [D loss: 1.644267] [G loss: 0.759508]\n",
      "[Epoch 42/100] [Batch 5000/6350] [D loss: 1.667227] [G loss: 0.786378]\n",
      "[Epoch 42/100] [Batch 6000/6350] [D loss: 1.690822] [G loss: 0.774053]\n",
      "Saving models...\n",
      "[Epoch 43/100] [Batch 0/6350] [D loss: 1.775006] [G loss: 0.732180]\n",
      "[Epoch 43/100] [Batch 1000/6350] [D loss: 1.703971] [G loss: 0.791979]\n",
      "[Epoch 43/100] [Batch 2000/6350] [D loss: 1.703885] [G loss: 0.711242]\n",
      "[Epoch 43/100] [Batch 3000/6350] [D loss: 1.701406] [G loss: 0.765685]\n",
      "[Epoch 43/100] [Batch 4000/6350] [D loss: 1.726142] [G loss: 0.741028]\n",
      "[Epoch 43/100] [Batch 5000/6350] [D loss: 1.684466] [G loss: 0.764534]\n",
      "[Epoch 43/100] [Batch 6000/6350] [D loss: 1.660406] [G loss: 0.860797]\n",
      "Saving models...\n",
      "[Epoch 44/100] [Batch 0/6350] [D loss: 1.720979] [G loss: 0.812315]\n",
      "[Epoch 44/100] [Batch 1000/6350] [D loss: 1.673606] [G loss: 0.785597]\n",
      "[Epoch 44/100] [Batch 2000/6350] [D loss: 1.705493] [G loss: 0.751182]\n",
      "[Epoch 44/100] [Batch 3000/6350] [D loss: 1.726454] [G loss: 0.765367]\n",
      "[Epoch 44/100] [Batch 4000/6350] [D loss: 1.739594] [G loss: 0.754033]\n",
      "[Epoch 44/100] [Batch 5000/6350] [D loss: 1.773522] [G loss: 0.733824]\n",
      "[Epoch 44/100] [Batch 6000/6350] [D loss: 1.689533] [G loss: 0.777221]\n",
      "Saving models...\n",
      "[Epoch 45/100] [Batch 0/6350] [D loss: 1.738843] [G loss: 0.698060]\n",
      "[Epoch 45/100] [Batch 1000/6350] [D loss: 1.685944] [G loss: 0.708886]\n",
      "[Epoch 45/100] [Batch 2000/6350] [D loss: 1.685039] [G loss: 0.708390]\n",
      "[Epoch 45/100] [Batch 3000/6350] [D loss: 1.818372] [G loss: 0.714768]\n",
      "[Epoch 45/100] [Batch 4000/6350] [D loss: 1.778471] [G loss: 0.738140]\n",
      "[Epoch 45/100] [Batch 5000/6350] [D loss: 1.718045] [G loss: 0.818444]\n",
      "[Epoch 45/100] [Batch 6000/6350] [D loss: 1.678877] [G loss: 0.723958]\n",
      "Saving models...\n",
      "[Epoch 46/100] [Batch 0/6350] [D loss: 1.725396] [G loss: 0.716606]\n",
      "[Epoch 46/100] [Batch 1000/6350] [D loss: 1.690688] [G loss: 0.716698]\n",
      "[Epoch 46/100] [Batch 2000/6350] [D loss: 1.739506] [G loss: 0.717684]\n",
      "[Epoch 46/100] [Batch 3000/6350] [D loss: 1.859318] [G loss: 0.781935]\n",
      "[Epoch 46/100] [Batch 4000/6350] [D loss: 1.773658] [G loss: 0.755770]\n",
      "[Epoch 46/100] [Batch 5000/6350] [D loss: 1.718028] [G loss: 0.792279]\n",
      "[Epoch 46/100] [Batch 6000/6350] [D loss: 1.735613] [G loss: 0.730791]\n",
      "Saving models...\n",
      "[Epoch 47/100] [Batch 0/6350] [D loss: 1.658039] [G loss: 0.771682]\n",
      "[Epoch 47/100] [Batch 1000/6350] [D loss: 1.730752] [G loss: 0.731454]\n",
      "[Epoch 47/100] [Batch 2000/6350] [D loss: 1.685806] [G loss: 0.737943]\n",
      "[Epoch 47/100] [Batch 3000/6350] [D loss: 1.721377] [G loss: 0.723193]\n",
      "[Epoch 47/100] [Batch 4000/6350] [D loss: 1.798127] [G loss: 0.755554]\n",
      "[Epoch 47/100] [Batch 5000/6350] [D loss: 1.696150] [G loss: 0.744569]\n",
      "[Epoch 47/100] [Batch 6000/6350] [D loss: 1.675665] [G loss: 0.748984]\n",
      "Saving models...\n",
      "[Epoch 48/100] [Batch 0/6350] [D loss: 1.666820] [G loss: 0.734016]\n",
      "[Epoch 48/100] [Batch 1000/6350] [D loss: 1.712793] [G loss: 0.724929]\n",
      "[Epoch 48/100] [Batch 2000/6350] [D loss: 1.673390] [G loss: 0.749555]\n",
      "[Epoch 48/100] [Batch 3000/6350] [D loss: 1.719081] [G loss: 0.722496]\n",
      "[Epoch 48/100] [Batch 4000/6350] [D loss: 1.653204] [G loss: 0.752898]\n",
      "[Epoch 48/100] [Batch 5000/6350] [D loss: 1.654876] [G loss: 0.751683]\n",
      "[Epoch 48/100] [Batch 6000/6350] [D loss: 1.678200] [G loss: 0.731780]\n",
      "Saving models...\n",
      "[Epoch 49/100] [Batch 0/6350] [D loss: 1.720109] [G loss: 0.727876]\n",
      "[Epoch 49/100] [Batch 1000/6350] [D loss: 1.646703] [G loss: 0.746702]\n",
      "[Epoch 49/100] [Batch 2000/6350] [D loss: 1.658103] [G loss: 0.739363]\n",
      "[Epoch 49/100] [Batch 3000/6350] [D loss: 1.678469] [G loss: 0.741195]\n",
      "[Epoch 49/100] [Batch 4000/6350] [D loss: 1.689384] [G loss: 0.796741]\n",
      "[Epoch 49/100] [Batch 5000/6350] [D loss: 2.539206] [G loss: 0.710888]\n",
      "[Epoch 49/100] [Batch 6000/6350] [D loss: 2.066361] [G loss: 0.642893]\n",
      "Saving models...\n",
      "[Epoch 50/100] [Batch 0/6350] [D loss: 2.188739] [G loss: 0.842674]\n",
      "[Epoch 50/100] [Batch 1000/6350] [D loss: 1.720419] [G loss: 0.699852]\n",
      "[Epoch 50/100] [Batch 2000/6350] [D loss: 1.800706] [G loss: 0.823910]\n",
      "[Epoch 50/100] [Batch 3000/6350] [D loss: 1.738375] [G loss: 0.776780]\n",
      "[Epoch 50/100] [Batch 4000/6350] [D loss: 1.803525] [G loss: 0.737453]\n",
      "[Epoch 50/100] [Batch 5000/6350] [D loss: 1.708831] [G loss: 0.777034]\n",
      "[Epoch 50/100] [Batch 6000/6350] [D loss: 1.713776] [G loss: 0.813566]\n",
      "Saving models...\n",
      "[Epoch 51/100] [Batch 0/6350] [D loss: 1.726515] [G loss: 0.746882]\n",
      "[Epoch 51/100] [Batch 1000/6350] [D loss: 1.756304] [G loss: 0.727784]\n",
      "[Epoch 51/100] [Batch 2000/6350] [D loss: 1.710622] [G loss: 0.773787]\n",
      "[Epoch 51/100] [Batch 3000/6350] [D loss: 1.662832] [G loss: 0.763450]\n",
      "[Epoch 51/100] [Batch 4000/6350] [D loss: 1.675375] [G loss: 0.749608]\n",
      "[Epoch 51/100] [Batch 5000/6350] [D loss: 1.690759] [G loss: 0.746244]\n",
      "[Epoch 51/100] [Batch 6000/6350] [D loss: 1.715685] [G loss: 0.783694]\n",
      "Saving models...\n",
      "[Epoch 52/100] [Batch 0/6350] [D loss: 1.643860] [G loss: 0.746851]\n",
      "[Epoch 52/100] [Batch 1000/6350] [D loss: 1.668910] [G loss: 0.760356]\n",
      "[Epoch 52/100] [Batch 2000/6350] [D loss: 1.701102] [G loss: 0.762844]\n",
      "[Epoch 52/100] [Batch 3000/6350] [D loss: 2.184134] [G loss: 0.535857]\n",
      "[Epoch 52/100] [Batch 4000/6350] [D loss: 1.756216] [G loss: 0.723233]\n",
      "[Epoch 52/100] [Batch 5000/6350] [D loss: 1.714831] [G loss: 0.813501]\n",
      "[Epoch 52/100] [Batch 6000/6350] [D loss: 1.702649] [G loss: 0.800587]\n",
      "Saving models...\n",
      "[Epoch 53/100] [Batch 0/6350] [D loss: 1.662473] [G loss: 0.769348]\n",
      "[Epoch 53/100] [Batch 1000/6350] [D loss: 1.726285] [G loss: 0.816552]\n",
      "[Epoch 53/100] [Batch 2000/6350] [D loss: 1.768988] [G loss: 0.774035]\n",
      "[Epoch 53/100] [Batch 3000/6350] [D loss: 1.693501] [G loss: 0.780100]\n",
      "[Epoch 53/100] [Batch 4000/6350] [D loss: 1.704689] [G loss: 0.733078]\n",
      "[Epoch 53/100] [Batch 5000/6350] [D loss: 1.681311] [G loss: 0.723868]\n",
      "[Epoch 53/100] [Batch 6000/6350] [D loss: 1.768642] [G loss: 0.739616]\n",
      "Saving models...\n",
      "[Epoch 54/100] [Batch 0/6350] [D loss: 1.691760] [G loss: 0.756657]\n",
      "[Epoch 54/100] [Batch 1000/6350] [D loss: 1.679114] [G loss: 0.739333]\n",
      "[Epoch 54/100] [Batch 2000/6350] [D loss: 1.709460] [G loss: 0.746134]\n",
      "[Epoch 54/100] [Batch 3000/6350] [D loss: 2.111489] [G loss: 0.619969]\n",
      "[Epoch 54/100] [Batch 4000/6350] [D loss: 1.720193] [G loss: 0.744549]\n",
      "[Epoch 54/100] [Batch 5000/6350] [D loss: 1.666145] [G loss: 0.759207]\n",
      "[Epoch 54/100] [Batch 6000/6350] [D loss: 1.695575] [G loss: 0.813829]\n",
      "Saving models...\n",
      "[Epoch 55/100] [Batch 0/6350] [D loss: 1.728270] [G loss: 0.737010]\n",
      "[Epoch 55/100] [Batch 1000/6350] [D loss: 1.717724] [G loss: 0.748309]\n",
      "[Epoch 55/100] [Batch 2000/6350] [D loss: 1.726786] [G loss: 0.755549]\n",
      "[Epoch 55/100] [Batch 3000/6350] [D loss: 1.740928] [G loss: 0.756631]\n",
      "[Epoch 55/100] [Batch 4000/6350] [D loss: 1.701222] [G loss: 0.778865]\n",
      "[Epoch 55/100] [Batch 5000/6350] [D loss: 1.708657] [G loss: 0.747489]\n",
      "[Epoch 55/100] [Batch 6000/6350] [D loss: 1.729244] [G loss: 0.761813]\n",
      "Saving models...\n",
      "[Epoch 56/100] [Batch 0/6350] [D loss: 1.718638] [G loss: 0.777449]\n",
      "[Epoch 56/100] [Batch 1000/6350] [D loss: 1.754998] [G loss: 0.750095]\n",
      "[Epoch 56/100] [Batch 2000/6350] [D loss: 1.733581] [G loss: 0.747337]\n",
      "[Epoch 56/100] [Batch 3000/6350] [D loss: 1.700431] [G loss: 0.757817]\n",
      "[Epoch 56/100] [Batch 4000/6350] [D loss: 1.661316] [G loss: 0.746500]\n",
      "[Epoch 56/100] [Batch 5000/6350] [D loss: 1.686341] [G loss: 0.760511]\n",
      "[Epoch 56/100] [Batch 6000/6350] [D loss: 1.730538] [G loss: 0.742574]\n",
      "Saving models...\n",
      "[Epoch 57/100] [Batch 0/6350] [D loss: 1.722219] [G loss: 0.720214]\n",
      "[Epoch 57/100] [Batch 1000/6350] [D loss: 1.722155] [G loss: 0.732762]\n",
      "[Epoch 57/100] [Batch 2000/6350] [D loss: 1.658169] [G loss: 0.747108]\n",
      "[Epoch 57/100] [Batch 3000/6350] [D loss: 1.701141] [G loss: 0.742000]\n",
      "[Epoch 57/100] [Batch 4000/6350] [D loss: 1.687670] [G loss: 0.740035]\n",
      "[Epoch 57/100] [Batch 5000/6350] [D loss: 1.675946] [G loss: 0.773405]\n",
      "[Epoch 57/100] [Batch 6000/6350] [D loss: 1.696446] [G loss: 0.735029]\n",
      "Saving models...\n",
      "[Epoch 58/100] [Batch 0/6350] [D loss: 1.671367] [G loss: 0.742593]\n",
      "[Epoch 58/100] [Batch 1000/6350] [D loss: 1.709768] [G loss: 0.759698]\n",
      "[Epoch 58/100] [Batch 2000/6350] [D loss: 1.732679] [G loss: 0.759270]\n",
      "[Epoch 58/100] [Batch 3000/6350] [D loss: 1.707584] [G loss: 0.720143]\n",
      "[Epoch 58/100] [Batch 4000/6350] [D loss: 1.707165] [G loss: 0.748370]\n",
      "[Epoch 58/100] [Batch 5000/6350] [D loss: 1.673747] [G loss: 0.764303]\n",
      "[Epoch 58/100] [Batch 6000/6350] [D loss: 1.680884] [G loss: 0.737382]\n",
      "Saving models...\n",
      "[Epoch 59/100] [Batch 0/6350] [D loss: 1.731256] [G loss: 0.738472]\n",
      "[Epoch 59/100] [Batch 1000/6350] [D loss: 1.699723] [G loss: 0.734135]\n",
      "[Epoch 59/100] [Batch 2000/6350] [D loss: 1.745556] [G loss: 0.745176]\n",
      "[Epoch 59/100] [Batch 3000/6350] [D loss: 1.686174] [G loss: 0.767187]\n",
      "[Epoch 59/100] [Batch 4000/6350] [D loss: 1.741349] [G loss: 0.723245]\n",
      "[Epoch 59/100] [Batch 5000/6350] [D loss: 1.712299] [G loss: 0.726901]\n",
      "[Epoch 59/100] [Batch 6000/6350] [D loss: 1.734899] [G loss: 0.771517]\n",
      "Saving models...\n",
      "[Epoch 60/100] [Batch 0/6350] [D loss: 1.710475] [G loss: 0.765402]\n",
      "[Epoch 60/100] [Batch 1000/6350] [D loss: 1.713695] [G loss: 0.702133]\n",
      "[Epoch 60/100] [Batch 2000/6350] [D loss: 1.685891] [G loss: 0.736933]\n",
      "[Epoch 60/100] [Batch 3000/6350] [D loss: 1.708598] [G loss: 0.748535]\n",
      "[Epoch 60/100] [Batch 4000/6350] [D loss: 1.677340] [G loss: 0.741304]\n",
      "[Epoch 60/100] [Batch 5000/6350] [D loss: 1.730227] [G loss: 0.720229]\n",
      "[Epoch 60/100] [Batch 6000/6350] [D loss: 1.678372] [G loss: 0.726382]\n",
      "Saving models...\n",
      "[Epoch 61/100] [Batch 0/6350] [D loss: 1.675453] [G loss: 0.711349]\n",
      "[Epoch 61/100] [Batch 1000/6350] [D loss: 1.670077] [G loss: 0.728098]\n",
      "[Epoch 61/100] [Batch 2000/6350] [D loss: 1.843859] [G loss: 0.716358]\n",
      "[Epoch 61/100] [Batch 3000/6350] [D loss: 1.694855] [G loss: 0.737868]\n",
      "[Epoch 61/100] [Batch 4000/6350] [D loss: 1.676323] [G loss: 0.750320]\n",
      "[Epoch 61/100] [Batch 5000/6350] [D loss: 1.690425] [G loss: 0.758680]\n",
      "[Epoch 61/100] [Batch 6000/6350] [D loss: 1.771644] [G loss: 0.727062]\n",
      "Saving models...\n",
      "[Epoch 62/100] [Batch 0/6350] [D loss: 1.693658] [G loss: 0.747117]\n",
      "[Epoch 62/100] [Batch 1000/6350] [D loss: 1.695296] [G loss: 0.736843]\n",
      "[Epoch 62/100] [Batch 2000/6350] [D loss: 1.681818] [G loss: 0.735217]\n",
      "[Epoch 62/100] [Batch 3000/6350] [D loss: 1.681164] [G loss: 0.731364]\n",
      "[Epoch 62/100] [Batch 4000/6350] [D loss: 1.720352] [G loss: 0.725414]\n",
      "[Epoch 62/100] [Batch 5000/6350] [D loss: 1.691717] [G loss: 0.716247]\n",
      "[Epoch 62/100] [Batch 6000/6350] [D loss: 1.696154] [G loss: 0.729349]\n",
      "Saving models...\n",
      "[Epoch 63/100] [Batch 0/6350] [D loss: 1.697717] [G loss: 0.746245]\n",
      "[Epoch 63/100] [Batch 1000/6350] [D loss: 1.689204] [G loss: 0.730515]\n",
      "[Epoch 63/100] [Batch 2000/6350] [D loss: 1.719412] [G loss: 0.720355]\n",
      "[Epoch 63/100] [Batch 3000/6350] [D loss: 1.678174] [G loss: 0.734612]\n",
      "[Epoch 63/100] [Batch 4000/6350] [D loss: 1.738729] [G loss: 0.759657]\n",
      "[Epoch 63/100] [Batch 5000/6350] [D loss: 1.735926] [G loss: 0.754221]\n",
      "[Epoch 63/100] [Batch 6000/6350] [D loss: 1.710387] [G loss: 0.775130]\n",
      "Saving models...\n",
      "[Epoch 64/100] [Batch 0/6350] [D loss: 1.664655] [G loss: 0.740965]\n",
      "[Epoch 64/100] [Batch 1000/6350] [D loss: 1.704088] [G loss: 0.752231]\n",
      "[Epoch 64/100] [Batch 2000/6350] [D loss: 1.700354] [G loss: 0.758471]\n",
      "[Epoch 64/100] [Batch 3000/6350] [D loss: 1.714304] [G loss: 0.726400]\n",
      "[Epoch 64/100] [Batch 4000/6350] [D loss: 1.697062] [G loss: 0.726528]\n",
      "[Epoch 64/100] [Batch 5000/6350] [D loss: 1.709501] [G loss: 0.759663]\n",
      "[Epoch 64/100] [Batch 6000/6350] [D loss: 1.719969] [G loss: 0.736307]\n",
      "Saving models...\n",
      "[Epoch 65/100] [Batch 0/6350] [D loss: 1.703424] [G loss: 0.744728]\n",
      "[Epoch 65/100] [Batch 1000/6350] [D loss: 1.677237] [G loss: 0.733941]\n",
      "[Epoch 65/100] [Batch 2000/6350] [D loss: 1.692246] [G loss: 0.729242]\n",
      "[Epoch 65/100] [Batch 3000/6350] [D loss: 1.634488] [G loss: 0.738716]\n",
      "[Epoch 65/100] [Batch 4000/6350] [D loss: 1.663789] [G loss: 0.744850]\n",
      "[Epoch 65/100] [Batch 5000/6350] [D loss: 1.626684] [G loss: 0.779907]\n",
      "[Epoch 65/100] [Batch 6000/6350] [D loss: 1.646032] [G loss: 0.785057]\n",
      "Saving models...\n",
      "[Epoch 66/100] [Batch 0/6350] [D loss: 1.699025] [G loss: 0.802722]\n",
      "[Epoch 66/100] [Batch 1000/6350] [D loss: 1.884622] [G loss: 0.705003]\n",
      "[Epoch 66/100] [Batch 2000/6350] [D loss: 1.617573] [G loss: 0.766892]\n",
      "[Epoch 66/100] [Batch 3000/6350] [D loss: 1.685825] [G loss: 0.821136]\n",
      "[Epoch 66/100] [Batch 4000/6350] [D loss: 1.665268] [G loss: 0.860891]\n",
      "[Epoch 66/100] [Batch 5000/6350] [D loss: 1.637282] [G loss: 0.777068]\n",
      "[Epoch 66/100] [Batch 6000/6350] [D loss: 1.650358] [G loss: 0.767740]\n",
      "Saving models...\n",
      "[Epoch 67/100] [Batch 0/6350] [D loss: 1.630562] [G loss: 0.805362]\n",
      "[Epoch 67/100] [Batch 1000/6350] [D loss: 1.687879] [G loss: 0.785117]\n",
      "[Epoch 67/100] [Batch 2000/6350] [D loss: 1.694576] [G loss: 0.769054]\n",
      "[Epoch 67/100] [Batch 3000/6350] [D loss: 1.705075] [G loss: 0.790831]\n",
      "[Epoch 67/100] [Batch 4000/6350] [D loss: 1.680629] [G loss: 0.749477]\n",
      "[Epoch 67/100] [Batch 5000/6350] [D loss: 1.672563] [G loss: 0.739048]\n",
      "[Epoch 67/100] [Batch 6000/6350] [D loss: 1.728385] [G loss: 0.731532]\n",
      "Saving models...\n",
      "[Epoch 68/100] [Batch 0/6350] [D loss: 1.665751] [G loss: 0.768376]\n",
      "[Epoch 68/100] [Batch 1000/6350] [D loss: 1.638875] [G loss: 0.748120]\n",
      "[Epoch 68/100] [Batch 2000/6350] [D loss: 1.687979] [G loss: 0.743558]\n",
      "[Epoch 68/100] [Batch 3000/6350] [D loss: 1.675695] [G loss: 0.748393]\n",
      "[Epoch 68/100] [Batch 4000/6350] [D loss: 1.729110] [G loss: 0.747676]\n",
      "[Epoch 68/100] [Batch 5000/6350] [D loss: 1.634120] [G loss: 0.755147]\n",
      "[Epoch 68/100] [Batch 6000/6350] [D loss: 1.664007] [G loss: 0.773748]\n",
      "Saving models...\n",
      "[Epoch 69/100] [Batch 0/6350] [D loss: 1.729386] [G loss: 0.766585]\n",
      "[Epoch 69/100] [Batch 1000/6350] [D loss: 1.707800] [G loss: 0.759921]\n",
      "[Epoch 69/100] [Batch 2000/6350] [D loss: 1.685610] [G loss: 0.737208]\n",
      "[Epoch 69/100] [Batch 3000/6350] [D loss: 1.672612] [G loss: 0.782211]\n",
      "[Epoch 69/100] [Batch 4000/6350] [D loss: 1.720787] [G loss: 0.776199]\n",
      "[Epoch 69/100] [Batch 5000/6350] [D loss: 1.654223] [G loss: 0.760549]\n",
      "[Epoch 69/100] [Batch 6000/6350] [D loss: 1.733989] [G loss: 0.757160]\n",
      "Saving models...\n",
      "[Epoch 70/100] [Batch 0/6350] [D loss: 1.707931] [G loss: 0.763327]\n",
      "[Epoch 70/100] [Batch 1000/6350] [D loss: 1.664338] [G loss: 0.759780]\n",
      "[Epoch 70/100] [Batch 2000/6350] [D loss: 1.624116] [G loss: 0.761605]\n",
      "[Epoch 70/100] [Batch 3000/6350] [D loss: 1.751510] [G loss: 0.777825]\n",
      "[Epoch 70/100] [Batch 4000/6350] [D loss: 1.680841] [G loss: 0.783611]\n",
      "[Epoch 70/100] [Batch 5000/6350] [D loss: 1.662545] [G loss: 0.769332]\n",
      "[Epoch 70/100] [Batch 6000/6350] [D loss: 1.661155] [G loss: 0.760673]\n",
      "Saving models...\n",
      "[Epoch 71/100] [Batch 0/6350] [D loss: 1.674300] [G loss: 0.755388]\n",
      "[Epoch 71/100] [Batch 1000/6350] [D loss: 1.652707] [G loss: 0.906790]\n",
      "[Epoch 71/100] [Batch 2000/6350] [D loss: 1.629204] [G loss: 0.780507]\n",
      "[Epoch 71/100] [Batch 3000/6350] [D loss: 1.655954] [G loss: 0.763852]\n",
      "[Epoch 71/100] [Batch 4000/6350] [D loss: 1.645288] [G loss: 0.761907]\n",
      "[Epoch 71/100] [Batch 5000/6350] [D loss: 1.645427] [G loss: 0.773355]\n",
      "[Epoch 71/100] [Batch 6000/6350] [D loss: 1.682930] [G loss: 0.774628]\n",
      "Saving models...\n",
      "[Epoch 72/100] [Batch 0/6350] [D loss: 1.747484] [G loss: 0.804354]\n",
      "[Epoch 72/100] [Batch 1000/6350] [D loss: 1.653399] [G loss: 0.780230]\n",
      "[Epoch 72/100] [Batch 2000/6350] [D loss: 1.637060] [G loss: 0.775638]\n",
      "[Epoch 72/100] [Batch 3000/6350] [D loss: 1.704349] [G loss: 0.781726]\n",
      "[Epoch 72/100] [Batch 4000/6350] [D loss: 1.716069] [G loss: 0.780710]\n",
      "[Epoch 72/100] [Batch 5000/6350] [D loss: 1.639306] [G loss: 0.759922]\n",
      "[Epoch 72/100] [Batch 6000/6350] [D loss: 1.596727] [G loss: 0.771322]\n",
      "Saving models...\n",
      "[Epoch 73/100] [Batch 0/6350] [D loss: 1.615643] [G loss: 0.773162]\n",
      "[Epoch 73/100] [Batch 1000/6350] [D loss: 1.613264] [G loss: 0.779117]\n",
      "[Epoch 73/100] [Batch 2000/6350] [D loss: 1.612352] [G loss: 0.762253]\n",
      "[Epoch 73/100] [Batch 3000/6350] [D loss: 1.713444] [G loss: 0.801616]\n",
      "[Epoch 73/100] [Batch 4000/6350] [D loss: 1.604892] [G loss: 0.798544]\n",
      "[Epoch 73/100] [Batch 5000/6350] [D loss: 1.840104] [G loss: 0.778295]\n",
      "[Epoch 73/100] [Batch 6000/6350] [D loss: 1.649944] [G loss: 0.771157]\n",
      "Saving models...\n",
      "[Epoch 74/100] [Batch 0/6350] [D loss: 1.685110] [G loss: 0.760639]\n",
      "[Epoch 74/100] [Batch 1000/6350] [D loss: 1.608545] [G loss: 0.783690]\n",
      "[Epoch 74/100] [Batch 2000/6350] [D loss: 1.621910] [G loss: 0.799777]\n",
      "[Epoch 74/100] [Batch 3000/6350] [D loss: 2.653902] [G loss: 0.649405]\n",
      "[Epoch 74/100] [Batch 4000/6350] [D loss: 2.290205] [G loss: 0.871508]\n",
      "[Epoch 74/100] [Batch 5000/6350] [D loss: 1.711082] [G loss: 0.673219]\n",
      "[Epoch 74/100] [Batch 6000/6350] [D loss: 1.728905] [G loss: 0.717261]\n",
      "Saving models...\n",
      "[Epoch 75/100] [Batch 0/6350] [D loss: 1.699890] [G loss: 0.739100]\n",
      "[Epoch 75/100] [Batch 1000/6350] [D loss: 1.686596] [G loss: 0.732571]\n",
      "[Epoch 75/100] [Batch 2000/6350] [D loss: 10.285223] [G loss: 0.760618]\n",
      "[Epoch 75/100] [Batch 3000/6350] [D loss: 10.777719] [G loss: 0.603623]\n",
      "[Epoch 75/100] [Batch 4000/6350] [D loss: 12.703730] [G loss: 0.561806]\n",
      "[Epoch 75/100] [Batch 5000/6350] [D loss: 12.497844] [G loss: 0.492102]\n",
      "[Epoch 75/100] [Batch 6000/6350] [D loss: 15.849747] [G loss: 0.621565]\n",
      "Saving models...\n",
      "[Epoch 76/100] [Batch 0/6350] [D loss: 13.741693] [G loss: 0.626840]\n",
      "[Epoch 76/100] [Batch 1000/6350] [D loss: 5.987375] [G loss: 0.709441]\n",
      "[Epoch 76/100] [Batch 2000/6350] [D loss: 2.141288] [G loss: 0.916672]\n",
      "[Epoch 76/100] [Batch 3000/6350] [D loss: 1.906858] [G loss: 0.644941]\n",
      "[Epoch 76/100] [Batch 4000/6350] [D loss: 2.301525] [G loss: 0.687697]\n",
      "[Epoch 76/100] [Batch 5000/6350] [D loss: 2.316182] [G loss: 0.667370]\n",
      "[Epoch 76/100] [Batch 6000/6350] [D loss: 1.975214] [G loss: 0.641580]\n",
      "Saving models...\n",
      "[Epoch 77/100] [Batch 0/6350] [D loss: 1.949026] [G loss: 1.765000]\n",
      "[Epoch 77/100] [Batch 1000/6350] [D loss: 1.860134] [G loss: 0.633456]\n",
      "[Epoch 77/100] [Batch 2000/6350] [D loss: 2.257599] [G loss: 0.600367]\n",
      "[Epoch 77/100] [Batch 3000/6350] [D loss: 2.199794] [G loss: 0.708174]\n",
      "[Epoch 77/100] [Batch 4000/6350] [D loss: 1.829251] [G loss: 0.909564]\n",
      "[Epoch 77/100] [Batch 5000/6350] [D loss: 1.688049] [G loss: 0.800520]\n",
      "[Epoch 77/100] [Batch 6000/6350] [D loss: 2.305570] [G loss: 0.503879]\n",
      "Saving models...\n",
      "[Epoch 78/100] [Batch 0/6350] [D loss: 2.239001] [G loss: 0.525147]\n",
      "[Epoch 78/100] [Batch 1000/6350] [D loss: 2.657531] [G loss: 0.442484]\n",
      "[Epoch 78/100] [Batch 2000/6350] [D loss: 2.284884] [G loss: 0.451324]\n",
      "[Epoch 78/100] [Batch 3000/6350] [D loss: 1.968382] [G loss: 0.467958]\n",
      "[Epoch 78/100] [Batch 4000/6350] [D loss: 2.160532] [G loss: 0.708856]\n",
      "[Epoch 78/100] [Batch 5000/6350] [D loss: 6.086501] [G loss: 0.593846]\n",
      "[Epoch 78/100] [Batch 6000/6350] [D loss: 17.580870] [G loss: 0.596313]\n",
      "Saving models...\n",
      "[Epoch 79/100] [Batch 0/6350] [D loss: 7.860622] [G loss: 0.854174]\n",
      "[Epoch 79/100] [Batch 1000/6350] [D loss: 3.105048] [G loss: 0.370121]\n",
      "[Epoch 79/100] [Batch 2000/6350] [D loss: 2.018631] [G loss: 0.639333]\n",
      "[Epoch 79/100] [Batch 3000/6350] [D loss: 1.652050] [G loss: 1.078033]\n",
      "[Epoch 79/100] [Batch 4000/6350] [D loss: 1.654113] [G loss: 0.854165]\n",
      "[Epoch 79/100] [Batch 5000/6350] [D loss: 1.955048] [G loss: 0.539284]\n",
      "[Epoch 79/100] [Batch 6000/6350] [D loss: 2.181583] [G loss: 0.664538]\n",
      "Saving models...\n",
      "[Epoch 80/100] [Batch 0/6350] [D loss: 1.730337] [G loss: 0.710037]\n",
      "[Epoch 80/100] [Batch 1000/6350] [D loss: 1.886712] [G loss: 0.814193]\n",
      "[Epoch 80/100] [Batch 2000/6350] [D loss: 1.822178] [G loss: 0.645461]\n",
      "[Epoch 80/100] [Batch 3000/6350] [D loss: 1.749561] [G loss: 0.660037]\n",
      "[Epoch 80/100] [Batch 4000/6350] [D loss: 1.575394] [G loss: 0.784283]\n",
      "[Epoch 80/100] [Batch 5000/6350] [D loss: 1.708961] [G loss: 0.654679]\n",
      "[Epoch 80/100] [Batch 6000/6350] [D loss: 1.564910] [G loss: 0.901979]\n",
      "Saving models...\n",
      "[Epoch 81/100] [Batch 0/6350] [D loss: 1.604852] [G loss: 0.938122]\n",
      "[Epoch 81/100] [Batch 1000/6350] [D loss: 1.585774] [G loss: 0.873686]\n",
      "[Epoch 81/100] [Batch 2000/6350] [D loss: 1.567830] [G loss: 0.947218]\n",
      "[Epoch 81/100] [Batch 3000/6350] [D loss: 1.559266] [G loss: 0.957344]\n",
      "[Epoch 81/100] [Batch 4000/6350] [D loss: 1.532813] [G loss: 0.898666]\n",
      "[Epoch 81/100] [Batch 5000/6350] [D loss: 1.561333] [G loss: 0.891473]\n",
      "[Epoch 81/100] [Batch 6000/6350] [D loss: 1.710961] [G loss: 0.934402]\n",
      "Saving models...\n",
      "[Epoch 82/100] [Batch 0/6350] [D loss: 1.578124] [G loss: 0.900640]\n",
      "[Epoch 82/100] [Batch 1000/6350] [D loss: 1.591595] [G loss: 0.824703]\n",
      "[Epoch 82/100] [Batch 2000/6350] [D loss: 1.596374] [G loss: 0.847491]\n",
      "[Epoch 82/100] [Batch 3000/6350] [D loss: 1.610601] [G loss: 0.839963]\n",
      "[Epoch 82/100] [Batch 4000/6350] [D loss: 1.512000] [G loss: 0.848597]\n",
      "[Epoch 82/100] [Batch 5000/6350] [D loss: 1.533880] [G loss: 0.929948]\n",
      "[Epoch 82/100] [Batch 6000/6350] [D loss: 1.627400] [G loss: 0.903819]\n",
      "Saving models...\n",
      "[Epoch 83/100] [Batch 0/6350] [D loss: 1.502147] [G loss: 0.923898]\n",
      "[Epoch 83/100] [Batch 1000/6350] [D loss: 1.454540] [G loss: 0.902454]\n",
      "[Epoch 83/100] [Batch 2000/6350] [D loss: 1.602466] [G loss: 0.925093]\n",
      "[Epoch 83/100] [Batch 3000/6350] [D loss: 1.598953] [G loss: 0.917505]\n",
      "[Epoch 83/100] [Batch 4000/6350] [D loss: 1.512868] [G loss: 0.899979]\n",
      "[Epoch 83/100] [Batch 5000/6350] [D loss: 1.657991] [G loss: 0.885929]\n",
      "[Epoch 83/100] [Batch 6000/6350] [D loss: 1.638395] [G loss: 0.837236]\n",
      "Saving models...\n",
      "[Epoch 84/100] [Batch 0/6350] [D loss: 1.666566] [G loss: 0.878263]\n",
      "[Epoch 84/100] [Batch 1000/6350] [D loss: 1.648174] [G loss: 0.988196]\n",
      "[Epoch 84/100] [Batch 2000/6350] [D loss: 1.600779] [G loss: 0.859524]\n",
      "[Epoch 84/100] [Batch 3000/6350] [D loss: 1.674687] [G loss: 0.873353]\n",
      "[Epoch 84/100] [Batch 4000/6350] [D loss: 1.671251] [G loss: 0.897842]\n",
      "[Epoch 84/100] [Batch 5000/6350] [D loss: 1.671327] [G loss: 0.807490]\n",
      "[Epoch 84/100] [Batch 6000/6350] [D loss: 1.651981] [G loss: 0.842120]\n",
      "Saving models...\n",
      "[Epoch 85/100] [Batch 0/6350] [D loss: 1.712514] [G loss: 0.825223]\n",
      "[Epoch 85/100] [Batch 1000/6350] [D loss: 1.663378] [G loss: 0.836089]\n",
      "[Epoch 85/100] [Batch 2000/6350] [D loss: 1.676328] [G loss: 0.850734]\n",
      "[Epoch 85/100] [Batch 3000/6350] [D loss: 1.648420] [G loss: 0.806659]\n",
      "[Epoch 85/100] [Batch 4000/6350] [D loss: 1.615726] [G loss: 0.811106]\n",
      "[Epoch 85/100] [Batch 5000/6350] [D loss: 1.728991] [G loss: 0.847643]\n",
      "[Epoch 85/100] [Batch 6000/6350] [D loss: 1.637197] [G loss: 0.791211]\n",
      "Saving models...\n",
      "[Epoch 86/100] [Batch 0/6350] [D loss: 1.704087] [G loss: 0.819814]\n",
      "[Epoch 86/100] [Batch 1000/6350] [D loss: 1.647078] [G loss: 0.830347]\n",
      "[Epoch 86/100] [Batch 2000/6350] [D loss: 1.683666] [G loss: 0.844196]\n",
      "[Epoch 86/100] [Batch 3000/6350] [D loss: 1.694053] [G loss: 0.808683]\n",
      "[Epoch 86/100] [Batch 4000/6350] [D loss: 1.717242] [G loss: 0.781989]\n",
      "[Epoch 86/100] [Batch 5000/6350] [D loss: 1.727018] [G loss: 0.792993]\n",
      "[Epoch 86/100] [Batch 6000/6350] [D loss: 1.699950] [G loss: 0.806642]\n",
      "Saving models...\n",
      "[Epoch 87/100] [Batch 0/6350] [D loss: 1.713765] [G loss: 0.794047]\n",
      "[Epoch 87/100] [Batch 1000/6350] [D loss: 1.699649] [G loss: 0.923685]\n",
      "[Epoch 87/100] [Batch 2000/6350] [D loss: 1.713381] [G loss: 0.722343]\n",
      "[Epoch 87/100] [Batch 3000/6350] [D loss: 1.688507] [G loss: 0.784784]\n",
      "[Epoch 87/100] [Batch 4000/6350] [D loss: 1.660830] [G loss: 0.731538]\n",
      "[Epoch 87/100] [Batch 5000/6350] [D loss: 1.721248] [G loss: 0.738378]\n",
      "[Epoch 87/100] [Batch 6000/6350] [D loss: 1.778751] [G loss: 0.730730]\n",
      "Saving models...\n",
      "[Epoch 88/100] [Batch 0/6350] [D loss: 1.699390] [G loss: 0.731808]\n",
      "[Epoch 88/100] [Batch 1000/6350] [D loss: 1.694648] [G loss: 0.732954]\n",
      "[Epoch 88/100] [Batch 2000/6350] [D loss: 1.701961] [G loss: 0.755099]\n",
      "[Epoch 88/100] [Batch 3000/6350] [D loss: 1.698169] [G loss: 0.744237]\n",
      "[Epoch 88/100] [Batch 4000/6350] [D loss: 1.678080] [G loss: 0.766560]\n",
      "[Epoch 88/100] [Batch 5000/6350] [D loss: 1.693598] [G loss: 0.740712]\n",
      "[Epoch 88/100] [Batch 6000/6350] [D loss: 1.686500] [G loss: 0.732377]\n",
      "Saving models...\n",
      "[Epoch 89/100] [Batch 0/6350] [D loss: 1.708831] [G loss: 0.726780]\n",
      "[Epoch 89/100] [Batch 1000/6350] [D loss: 1.704944] [G loss: 0.722082]\n",
      "[Epoch 89/100] [Batch 2000/6350] [D loss: 1.703128] [G loss: 0.730093]\n",
      "[Epoch 89/100] [Batch 3000/6350] [D loss: 1.681790] [G loss: 0.725121]\n",
      "[Epoch 89/100] [Batch 4000/6350] [D loss: 1.717367] [G loss: 0.768654]\n",
      "[Epoch 89/100] [Batch 5000/6350] [D loss: 1.707385] [G loss: 0.747353]\n",
      "[Epoch 89/100] [Batch 6000/6350] [D loss: 1.691153] [G loss: 0.743804]\n",
      "Saving models...\n",
      "[Epoch 90/100] [Batch 0/6350] [D loss: 1.709249] [G loss: 0.742939]\n",
      "[Epoch 90/100] [Batch 1000/6350] [D loss: 1.676468] [G loss: 0.726271]\n",
      "[Epoch 90/100] [Batch 2000/6350] [D loss: 1.722904] [G loss: 0.737150]\n",
      "[Epoch 90/100] [Batch 3000/6350] [D loss: 1.718149] [G loss: 0.730063]\n",
      "[Epoch 90/100] [Batch 4000/6350] [D loss: 1.688305] [G loss: 0.733809]\n",
      "[Epoch 90/100] [Batch 5000/6350] [D loss: 1.680338] [G loss: 0.744509]\n",
      "[Epoch 90/100] [Batch 6000/6350] [D loss: 1.687524] [G loss: 0.741545]\n",
      "Saving models...\n",
      "[Epoch 91/100] [Batch 0/6350] [D loss: 1.716506] [G loss: 0.716684]\n",
      "[Epoch 91/100] [Batch 1000/6350] [D loss: 1.729956] [G loss: 0.746968]\n",
      "[Epoch 91/100] [Batch 2000/6350] [D loss: 1.736289] [G loss: 0.717936]\n",
      "[Epoch 91/100] [Batch 3000/6350] [D loss: 1.766485] [G loss: 0.721746]\n",
      "[Epoch 91/100] [Batch 4000/6350] [D loss: 1.686628] [G loss: 0.712517]\n",
      "[Epoch 91/100] [Batch 5000/6350] [D loss: 1.770026] [G loss: 0.724629]\n",
      "[Epoch 91/100] [Batch 6000/6350] [D loss: 1.675077] [G loss: 0.726703]\n",
      "Saving models...\n",
      "[Epoch 92/100] [Batch 0/6350] [D loss: 1.738545] [G loss: 0.725245]\n",
      "[Epoch 92/100] [Batch 1000/6350] [D loss: 1.707203] [G loss: 0.722925]\n",
      "[Epoch 92/100] [Batch 2000/6350] [D loss: 1.727893] [G loss: 0.707706]\n",
      "[Epoch 92/100] [Batch 3000/6350] [D loss: 1.695561] [G loss: 0.714629]\n",
      "[Epoch 92/100] [Batch 4000/6350] [D loss: 1.680851] [G loss: 0.727629]\n",
      "[Epoch 92/100] [Batch 5000/6350] [D loss: 1.706239] [G loss: 0.704043]\n",
      "[Epoch 92/100] [Batch 6000/6350] [D loss: 1.758412] [G loss: 0.697712]\n",
      "Saving models...\n",
      "[Epoch 93/100] [Batch 0/6350] [D loss: 1.770044] [G loss: 0.727816]\n",
      "[Epoch 93/100] [Batch 1000/6350] [D loss: 1.735235] [G loss: 0.727444]\n",
      "[Epoch 93/100] [Batch 2000/6350] [D loss: 1.704762] [G loss: 0.717159]\n",
      "[Epoch 93/100] [Batch 3000/6350] [D loss: 1.721696] [G loss: 0.722975]\n",
      "[Epoch 93/100] [Batch 4000/6350] [D loss: 1.697302] [G loss: 0.714580]\n",
      "[Epoch 93/100] [Batch 5000/6350] [D loss: 1.738002] [G loss: 0.724130]\n",
      "[Epoch 93/100] [Batch 6000/6350] [D loss: 1.717747] [G loss: 0.731258]\n",
      "Saving models...\n",
      "[Epoch 94/100] [Batch 0/6350] [D loss: 1.736873] [G loss: 0.721199]\n",
      "[Epoch 94/100] [Batch 1000/6350] [D loss: 1.787155] [G loss: 0.737949]\n",
      "[Epoch 94/100] [Batch 2000/6350] [D loss: 1.723276] [G loss: 0.759797]\n",
      "[Epoch 94/100] [Batch 3000/6350] [D loss: 1.733717] [G loss: 0.723865]\n",
      "[Epoch 94/100] [Batch 4000/6350] [D loss: 1.672688] [G loss: 0.732965]\n",
      "[Epoch 94/100] [Batch 5000/6350] [D loss: 1.737535] [G loss: 0.738781]\n",
      "[Epoch 94/100] [Batch 6000/6350] [D loss: 1.739326] [G loss: 0.713814]\n",
      "Saving models...\n",
      "[Epoch 95/100] [Batch 0/6350] [D loss: 1.689272] [G loss: 0.718438]\n",
      "[Epoch 95/100] [Batch 1000/6350] [D loss: 1.725187] [G loss: 0.699056]\n",
      "[Epoch 95/100] [Batch 2000/6350] [D loss: 1.687022] [G loss: 0.718046]\n",
      "[Epoch 95/100] [Batch 3000/6350] [D loss: 1.716663] [G loss: 0.711908]\n",
      "[Epoch 95/100] [Batch 4000/6350] [D loss: 1.740169] [G loss: 0.713007]\n",
      "[Epoch 95/100] [Batch 5000/6350] [D loss: 1.745038] [G loss: 0.715228]\n",
      "[Epoch 95/100] [Batch 6000/6350] [D loss: 1.681391] [G loss: 0.724182]\n",
      "Saving models...\n",
      "[Epoch 96/100] [Batch 0/6350] [D loss: 1.712513] [G loss: 0.737412]\n",
      "[Epoch 96/100] [Batch 1000/6350] [D loss: 1.683129] [G loss: 0.727588]\n",
      "[Epoch 96/100] [Batch 2000/6350] [D loss: 1.716469] [G loss: 0.714553]\n",
      "[Epoch 96/100] [Batch 3000/6350] [D loss: 1.686271] [G loss: 0.720399]\n",
      "[Epoch 96/100] [Batch 4000/6350] [D loss: 1.673193] [G loss: 0.730977]\n",
      "[Epoch 96/100] [Batch 5000/6350] [D loss: 1.733438] [G loss: 0.718755]\n",
      "[Epoch 96/100] [Batch 6000/6350] [D loss: 1.757264] [G loss: 0.720227]\n",
      "Saving models...\n",
      "[Epoch 97/100] [Batch 0/6350] [D loss: 1.762438] [G loss: 0.749145]\n",
      "[Epoch 97/100] [Batch 1000/6350] [D loss: 1.701620] [G loss: 0.745037]\n",
      "[Epoch 97/100] [Batch 2000/6350] [D loss: 1.719918] [G loss: 0.716521]\n",
      "[Epoch 97/100] [Batch 3000/6350] [D loss: 1.687289] [G loss: 0.721505]\n",
      "[Epoch 97/100] [Batch 4000/6350] [D loss: 1.733568] [G loss: 0.722220]\n",
      "[Epoch 97/100] [Batch 5000/6350] [D loss: 1.716131] [G loss: 0.731411]\n",
      "[Epoch 97/100] [Batch 6000/6350] [D loss: 1.725621] [G loss: 0.730163]\n",
      "Saving models...\n",
      "[Epoch 98/100] [Batch 0/6350] [D loss: 1.704667] [G loss: 0.729109]\n",
      "[Epoch 98/100] [Batch 1000/6350] [D loss: 1.670722] [G loss: 0.733090]\n",
      "[Epoch 98/100] [Batch 2000/6350] [D loss: 1.689107] [G loss: 0.725906]\n",
      "[Epoch 98/100] [Batch 3000/6350] [D loss: 1.751154] [G loss: 0.718288]\n",
      "[Epoch 98/100] [Batch 4000/6350] [D loss: 1.657017] [G loss: 0.728396]\n",
      "[Epoch 98/100] [Batch 5000/6350] [D loss: 1.733884] [G loss: 0.717916]\n",
      "[Epoch 98/100] [Batch 6000/6350] [D loss: 2.581304] [G loss: 0.747724]\n",
      "Saving models...\n",
      "[Epoch 99/100] [Batch 0/6350] [D loss: 2.140607] [G loss: 0.694299]\n",
      "[Epoch 99/100] [Batch 1000/6350] [D loss: 2.877773] [G loss: 0.640646]\n",
      "[Epoch 99/100] [Batch 2000/6350] [D loss: 1.924589] [G loss: 0.661874]\n",
      "[Epoch 99/100] [Batch 3000/6350] [D loss: 1.901357] [G loss: 0.617204]\n",
      "[Epoch 99/100] [Batch 4000/6350] [D loss: 1.683867] [G loss: 0.764617]\n",
      "[Epoch 99/100] [Batch 5000/6350] [D loss: 1.720809] [G loss: 0.752926]\n",
      "[Epoch 99/100] [Batch 6000/6350] [D loss: 1.721471] [G loss: 0.719807]\n",
      "Saving models...\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (data, labels) in enumerate(dataloader):\n",
    "        batch_size = data.shape[0]\n",
    "        real_data = Variable(data.type(FloatTensor))        \n",
    "        labels = Variable(labels.float())\n",
    "        labels = labels.view(-1, 1).type(FloatTensor)\n",
    "        \n",
    "        # Train the discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        d_real_outputs = discriminator(real_data)\n",
    "        d_real_loss = adversarial_loss(d_real_outputs, labels)\n",
    "        \n",
    "        fake_data = generator(noise, 0)\n",
    "        d_fake_outputs = discriminator(fake_data.detach())\n",
    "        d_fake_loss1 = adversarial_loss(d_fake_outputs, torch.full((batch_size, 1), 0.4).type(FloatTensor))\n",
    "        \n",
    "        fake_data = generator(noise, 1)\n",
    "        d_fake_outputs = discriminator(fake_data.detach())\n",
    "        d_fake_loss2 = adversarial_loss(d_fake_outputs, torch.full((batch_size, 1), 0.6).type(FloatTensor))\n",
    "        \n",
    "        d_loss = d_real_loss + d_fake_loss1 + d_fake_loss2\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # Train the generator\n",
    "        optimizer_G.zero_grad()\n",
    "        noise = torch.randn(batch_size, latent_dim)\n",
    "        fake_data = generator(noise, 0)\n",
    "        d_fake_outputs = discriminator(fake_data)\n",
    "        targets = torch.full((batch_size, 1), 0)\n",
    "        g_loss1 = adversarial_loss(d_fake_outputs, targets.float())\n",
    "\n",
    "        noise = torch.randn(batch_size, latent_dim)\n",
    "        fake_data = generator(noise, 1)\n",
    "        d_fake_outputs = discriminator(fake_data)\n",
    "        targets = torch.full((batch_size, 1), 1)\n",
    "        g_loss2 = adversarial_loss(d_fake_outputs, targets.float())\n",
    "        \n",
    "        g_loss = g_loss1 + g_loss2\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "\n",
    "        # Print the loss for each epoch\n",
    "        if i % 1000 == 0:\n",
    "            print(\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\" % (epoch, num_epochs, i, len(dataloader), d_loss.item(), g_loss.item()))\n",
    "\n",
    "    print('Saving models...')\n",
    "    # save discriminator model used for classification\n",
    "    PATH_disc = './models/discr_model-'\n",
    "    PATH = PATH_disc + 'lr=' + str(lr) + '-batch=' + str(batch_size) + '-epochs=' + str(epoch) + '.pth'\n",
    "    torch.save(discriminator.state_dict(), PATH)\n",
    "    \n",
    "    # save generator model\n",
    "    PATH_gen = './models/gen_model-'\n",
    "    PATH = PATH_gen + 'lr=' + str(lr) + '-batch=' + str(batch_size) + '-epochs=' + str(epoch) + '.pth'\n",
    "    torch.save(generator.state_dict(), PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
