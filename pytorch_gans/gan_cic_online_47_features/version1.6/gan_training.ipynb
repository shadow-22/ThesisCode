{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d45dfea-b5ab-4ee0-81db-982a5de12b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch import FloatTensor, LongTensor\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "645898be-0d8e-400b-bf4d-6339c2253208",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['Destination Port', 'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Fwd Packets',\n",
    "                     'Total Length of Bwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean',\n",
    "                     'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', \n",
    "                     'Bwd Packet Length Std', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max',\n",
    "                     'Flow IAT Min', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Mean', 'Bwd IAT Std',\n",
    "                     'Bwd IAT Max', 'Bwd IAT Min', 'Min Packet Length', 'Max Packet Length', \n",
    "                     'Packet Length Mean', 'Packet Length Std', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', \n",
    "                     'PSH Flag Count', 'ACK Flag Count', 'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count']\n",
    "\n",
    "added_features = ['Fwd PSH Flags', 'Fwd URG Flags', 'Bwd PSH Flags', 'Bwd URG Flags']\n",
    "\n",
    "#extra_features = ['Active Max', 'Idle Max' ]\n",
    "\n",
    "computed_features = ['Flow Duration', 'Flow Bytes/s', 'Flow Packets/s', 'Fwd Packets/s', 'Bwd Packets/s', 'Packet Length Variance']\n",
    "\n",
    "selected_features_total = (selected_features + \n",
    "                           added_features + \n",
    "                           computed_features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf915d9a-c8de-4044-9c9b-3f63660fd549",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessor:\n",
    "    def __init__(self, file_path, selected_features):\n",
    "        self.input_dataset = pd.read_csv(file_path)\n",
    "        self.selected_features = selected_features\n",
    "        \n",
    "    def print_input_data(self):\n",
    "        print('The input dataset has the following columns/features')\n",
    "        print(self.input_dataset.columns)\n",
    "        labels = self.input_dataset['Label']\n",
    "        print('The labels are as follows')\n",
    "        print(labels.value_counts())\n",
    "        print(labels.value_counts().plot.pie())\n",
    "        \n",
    "    def preprocess(self):\n",
    "        X, y = self.feature_selection()\n",
    "        X = self.imputing(X)\n",
    "        X = self.scaling(X)\n",
    "        y = self.label_binarization(y)\n",
    "        self.data = torch.tensor(X)\n",
    "        self.targets = torch.tensor(y)\n",
    "        \n",
    "    def feature_selection(self):\n",
    "        X = self.input_dataset[self.selected_features]\n",
    "        y = self.input_dataset['Label']\n",
    "        return X, y\n",
    "    \n",
    "    def imputing(self, X):\n",
    "        X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        Imp = SimpleImputer()\n",
    "        X = Imp.fit_transform(X)\n",
    "        return X\n",
    "        \n",
    "    def scaling(self, X):\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "        joblib.dump(scaler, 'minmaxscaler')\n",
    "        return X\n",
    "    \n",
    "    def label_binarization(self, y):\n",
    "        for label in y:\n",
    "            if (label != 'BENIGN' and label != 'ATTACK'):\n",
    "                y.replace(label, 'ATTACK', inplace=True)\n",
    "        y.replace('BENIGN', 1, inplace=True)\n",
    "        y.replace('ATTACK', 0, inplace=True)\n",
    "        return y\n",
    "    \n",
    "    def return_data(self):\n",
    "        return self.data, self.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b3352fd-24e3-418b-ad69-5610a96dfc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_path, selected_features):\n",
    "        preprocessor = PreProcessor(file_path, selected_features)\n",
    "        preprocessor.print_input_data()\n",
    "        preprocessor.preprocess()\n",
    "        self.data, self.targets = preprocessor.return_data()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data[idx]\n",
    "        label = self.targets[idx]\n",
    "        return row, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90b14a53-341f-4950-9ebe-40901a12929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0319c25f-9fc6-4081-8431-c128778ea25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input dataset has the following columns/features\n",
      "Index(['Destination Port', 'Flow Duration', 'Total Fwd Packets',\n",
      "       'Total Backward Packets', 'Total Length of Fwd Packets',\n",
      "       'Total Length of Bwd Packets', 'Fwd Packet Length Max',\n",
      "       'Fwd Packet Length Min', 'Fwd Packet Length Mean',\n",
      "       'Fwd Packet Length Std', 'Bwd Packet Length Max',\n",
      "       'Bwd Packet Length Min', 'Bwd Packet Length Mean',\n",
      "       'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s',\n",
      "       'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min',\n",
      "       'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max',\n",
      "       'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std',\n",
      "       'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags',\n",
      "       'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length',\n",
      "       'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s',\n",
      "       'Min Packet Length', 'Max Packet Length', 'Packet Length Mean',\n",
      "       'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count',\n",
      "       'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count',\n",
      "       'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio',\n",
      "       'Average Packet Size', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size',\n",
      "       'Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk',\n",
      "       'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk',\n",
      "       'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', 'Subflow Fwd Bytes',\n",
      "       'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward',\n",
      "       'Init_Win_bytes_backward', 'act_data_pkt_fwd', 'min_seg_size_forward',\n",
      "       'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean',\n",
      "       'Idle Std', 'Idle Max', 'Idle Min', 'Label'],\n",
      "      dtype='object')\n",
      "The labels are as follows\n",
      "BENIGN    113747\n",
      "ATTACK     27790\n",
      "Name: Label, dtype: int64\n",
      "Axes(0.22375,0.11;0.5775x0.77)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katsa\\AppData\\Local\\Temp\\ipykernel_15880\\1543300877.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.replace([np.inf, -np.inf], np.nan, inplace=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_path = \"C:/Users/katsa/OneDrive/Jupyter_files/shallow_models_online/cic_train_sample_binary.csv\"\n",
    "CICDataset = MyDataset(sample_path, selected_features_total)\n",
    "batch_size = 64\n",
    "dataloader = DataLoader(CICDataset, batch_size=batch_size, \n",
    "                          shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e7f2c41-7fd2-4c2c-9558-911d2cf1f8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator model\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim + 1, 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(16, 24),\n",
    "            nn.BatchNorm1d(24, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(24, 32),\n",
    "            nn.BatchNorm1d(32, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(32, 40),\n",
    "            nn.BatchNorm1d(40, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(40, data_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z, label):\n",
    "        label = torch.full((batch_size, 1), label)\n",
    "        z = torch.cat((noise, label), 1)\n",
    "        data = self.model(z)\n",
    "        return data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a1b526a-6105-48fe-883f-3113f2480a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the discriminator model\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(data_dim, 40),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(40, 32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(8, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        validity = self.model(data)\n",
    "        return validity\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80938735-654d-4141-884f-d09bf28bf403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "# Definition of hyperparameters\n",
    "\n",
    "data_dim=len(selected_features_total)\n",
    "latent_dim = 8\n",
    "print(data_dim)\n",
    "lr = 0.0002\n",
    "adversarial_loss = nn.BCELoss()\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c663729-ff33-4ea9-a470-0e1cf2c7b886",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(batch_size, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "10da37bd-24f5-4771-8a28-918cb1772d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_lst = [5, 10, 15, 20, 25, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab3315ec-ec36-40ad-adf1-4b2a582b9ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/100] [Batch 0/2211] [D loss: 2.028075] [G loss: 1.402511]\n",
      "[Epoch 0/100] [Batch 1000/2211] [D loss: 1.677869] [G loss: 1.069430]\n",
      "[Epoch 0/100] [Batch 2000/2211] [D loss: 1.562885] [G loss: 0.937651]\n",
      "Saving models...\n",
      "[Epoch 1/100] [Batch 0/2211] [D loss: 1.591988] [G loss: 0.944359]\n",
      "[Epoch 1/100] [Batch 1000/2211] [D loss: 1.573730] [G loss: 0.921671]\n",
      "[Epoch 1/100] [Batch 2000/2211] [D loss: 1.664149] [G loss: 0.853418]\n",
      "Saving models...\n",
      "[Epoch 2/100] [Batch 0/2211] [D loss: 1.577153] [G loss: 0.842461]\n",
      "[Epoch 2/100] [Batch 1000/2211] [D loss: 1.627570] [G loss: 0.852552]\n",
      "[Epoch 2/100] [Batch 2000/2211] [D loss: 1.617634] [G loss: 0.846683]\n",
      "Saving models...\n",
      "[Epoch 3/100] [Batch 0/2211] [D loss: 1.630180] [G loss: 0.851040]\n",
      "[Epoch 3/100] [Batch 1000/2211] [D loss: 1.671743] [G loss: 0.831465]\n",
      "[Epoch 3/100] [Batch 2000/2211] [D loss: 1.684411] [G loss: 0.823968]\n",
      "Saving models...\n",
      "[Epoch 4/100] [Batch 0/2211] [D loss: 1.662930] [G loss: 0.819033]\n",
      "[Epoch 4/100] [Batch 1000/2211] [D loss: 1.700770] [G loss: 0.805445]\n",
      "[Epoch 4/100] [Batch 2000/2211] [D loss: 1.725448] [G loss: 0.842381]\n",
      "Saving models...\n",
      "[Epoch 5/100] [Batch 0/2211] [D loss: 1.587863] [G loss: 0.828919]\n",
      "[Epoch 5/100] [Batch 1000/2211] [D loss: 1.634974] [G loss: 0.814084]\n",
      "[Epoch 5/100] [Batch 2000/2211] [D loss: 1.800030] [G loss: 0.789639]\n",
      "Saving models...\n",
      "[Epoch 6/100] [Batch 0/2211] [D loss: 1.759889] [G loss: 0.802466]\n",
      "[Epoch 6/100] [Batch 1000/2211] [D loss: 1.693812] [G loss: 0.794516]\n",
      "[Epoch 6/100] [Batch 2000/2211] [D loss: 1.691491] [G loss: 0.829666]\n",
      "Saving models...\n",
      "[Epoch 7/100] [Batch 0/2211] [D loss: 1.688832] [G loss: 0.770335]\n",
      "[Epoch 7/100] [Batch 1000/2211] [D loss: 1.715406] [G loss: 0.801409]\n",
      "[Epoch 7/100] [Batch 2000/2211] [D loss: 1.697419] [G loss: 0.742787]\n",
      "Saving models...\n",
      "[Epoch 8/100] [Batch 0/2211] [D loss: 1.651846] [G loss: 0.792956]\n",
      "[Epoch 8/100] [Batch 1000/2211] [D loss: 1.592378] [G loss: 0.804098]\n",
      "[Epoch 8/100] [Batch 2000/2211] [D loss: 1.723604] [G loss: 0.832363]\n",
      "Saving models...\n",
      "[Epoch 9/100] [Batch 0/2211] [D loss: 1.682553] [G loss: 0.782135]\n",
      "[Epoch 9/100] [Batch 1000/2211] [D loss: 1.602449] [G loss: 0.856097]\n",
      "[Epoch 9/100] [Batch 2000/2211] [D loss: 1.709442] [G loss: 0.856379]\n",
      "Saving models...\n",
      "[Epoch 10/100] [Batch 0/2211] [D loss: 1.710024] [G loss: 0.819620]\n",
      "[Epoch 10/100] [Batch 1000/2211] [D loss: 1.711248] [G loss: 0.792735]\n",
      "[Epoch 10/100] [Batch 2000/2211] [D loss: 1.677160] [G loss: 0.836833]\n",
      "Saving models...\n",
      "[Epoch 11/100] [Batch 0/2211] [D loss: 1.609267] [G loss: 0.783797]\n",
      "[Epoch 11/100] [Batch 1000/2211] [D loss: 1.605933] [G loss: 0.766197]\n",
      "[Epoch 11/100] [Batch 2000/2211] [D loss: 1.659161] [G loss: 0.749075]\n",
      "Saving models...\n",
      "[Epoch 12/100] [Batch 0/2211] [D loss: 1.741864] [G loss: 0.800019]\n",
      "[Epoch 12/100] [Batch 1000/2211] [D loss: 1.631504] [G loss: 0.854475]\n",
      "[Epoch 12/100] [Batch 2000/2211] [D loss: 1.557162] [G loss: 0.892377]\n",
      "Saving models...\n",
      "[Epoch 13/100] [Batch 0/2211] [D loss: 1.526654] [G loss: 0.884120]\n",
      "[Epoch 13/100] [Batch 1000/2211] [D loss: 1.630360] [G loss: 0.918848]\n",
      "[Epoch 13/100] [Batch 2000/2211] [D loss: 1.580481] [G loss: 0.905274]\n",
      "Saving models...\n",
      "[Epoch 14/100] [Batch 0/2211] [D loss: 1.518566] [G loss: 0.917714]\n",
      "[Epoch 14/100] [Batch 1000/2211] [D loss: 1.531947] [G loss: 0.911599]\n",
      "[Epoch 14/100] [Batch 2000/2211] [D loss: 1.508979] [G loss: 0.936717]\n",
      "Saving models...\n",
      "[Epoch 15/100] [Batch 0/2211] [D loss: 1.463523] [G loss: 0.951726]\n",
      "[Epoch 15/100] [Batch 1000/2211] [D loss: 1.522240] [G loss: 1.020542]\n",
      "[Epoch 15/100] [Batch 2000/2211] [D loss: 1.534533] [G loss: 0.908959]\n",
      "Saving models...\n",
      "[Epoch 16/100] [Batch 0/2211] [D loss: 1.519926] [G loss: 0.858568]\n",
      "[Epoch 16/100] [Batch 1000/2211] [D loss: 1.578908] [G loss: 0.796262]\n",
      "[Epoch 16/100] [Batch 2000/2211] [D loss: 1.659263] [G loss: 0.933513]\n",
      "Saving models...\n",
      "[Epoch 17/100] [Batch 0/2211] [D loss: 1.613208] [G loss: 0.855778]\n",
      "[Epoch 17/100] [Batch 1000/2211] [D loss: 1.584370] [G loss: 0.904837]\n",
      "[Epoch 17/100] [Batch 2000/2211] [D loss: 1.619058] [G loss: 0.843562]\n",
      "Saving models...\n",
      "[Epoch 18/100] [Batch 0/2211] [D loss: 1.556100] [G loss: 0.780288]\n",
      "[Epoch 18/100] [Batch 1000/2211] [D loss: 1.602677] [G loss: 0.753732]\n",
      "[Epoch 18/100] [Batch 2000/2211] [D loss: 1.571217] [G loss: 0.871206]\n",
      "Saving models...\n",
      "[Epoch 19/100] [Batch 0/2211] [D loss: 1.614518] [G loss: 0.907148]\n",
      "[Epoch 19/100] [Batch 1000/2211] [D loss: 1.563228] [G loss: 0.951293]\n",
      "[Epoch 19/100] [Batch 2000/2211] [D loss: 1.570661] [G loss: 0.702583]\n",
      "Saving models...\n",
      "[Epoch 20/100] [Batch 0/2211] [D loss: 1.578870] [G loss: 0.806012]\n",
      "[Epoch 20/100] [Batch 1000/2211] [D loss: 1.690114] [G loss: 0.949097]\n",
      "[Epoch 20/100] [Batch 2000/2211] [D loss: 1.654870] [G loss: 0.854727]\n",
      "Saving models...\n",
      "[Epoch 21/100] [Batch 0/2211] [D loss: 2.188230] [G loss: 0.798820]\n",
      "[Epoch 21/100] [Batch 1000/2211] [D loss: 1.564755] [G loss: 0.889354]\n",
      "[Epoch 21/100] [Batch 2000/2211] [D loss: 1.543542] [G loss: 0.905786]\n",
      "Saving models...\n",
      "[Epoch 22/100] [Batch 0/2211] [D loss: 1.570825] [G loss: 0.851504]\n",
      "[Epoch 22/100] [Batch 1000/2211] [D loss: 1.691292] [G loss: 0.807967]\n",
      "[Epoch 22/100] [Batch 2000/2211] [D loss: 1.610844] [G loss: 0.833234]\n",
      "Saving models...\n",
      "[Epoch 23/100] [Batch 0/2211] [D loss: 1.643565] [G loss: 0.810538]\n",
      "[Epoch 23/100] [Batch 1000/2211] [D loss: 1.602610] [G loss: 0.793714]\n",
      "[Epoch 23/100] [Batch 2000/2211] [D loss: 1.633333] [G loss: 0.900943]\n",
      "Saving models...\n",
      "[Epoch 24/100] [Batch 0/2211] [D loss: 1.584262] [G loss: 0.848982]\n",
      "[Epoch 24/100] [Batch 1000/2211] [D loss: 1.619000] [G loss: 0.860337]\n",
      "[Epoch 24/100] [Batch 2000/2211] [D loss: 1.530061] [G loss: 0.840711]\n",
      "Saving models...\n",
      "[Epoch 25/100] [Batch 0/2211] [D loss: 1.583450] [G loss: 0.817665]\n",
      "[Epoch 25/100] [Batch 1000/2211] [D loss: 1.556901] [G loss: 0.827455]\n",
      "[Epoch 25/100] [Batch 2000/2211] [D loss: 1.568970] [G loss: 0.768720]\n",
      "Saving models...\n",
      "[Epoch 26/100] [Batch 0/2211] [D loss: 1.538851] [G loss: 0.867320]\n",
      "[Epoch 26/100] [Batch 1000/2211] [D loss: 1.566543] [G loss: 0.903542]\n",
      "[Epoch 26/100] [Batch 2000/2211] [D loss: 1.619885] [G loss: 0.929881]\n",
      "Saving models...\n",
      "[Epoch 27/100] [Batch 0/2211] [D loss: 1.563613] [G loss: 0.776197]\n",
      "[Epoch 27/100] [Batch 1000/2211] [D loss: 1.508886] [G loss: 0.870631]\n",
      "[Epoch 27/100] [Batch 2000/2211] [D loss: 1.526375] [G loss: 0.838991]\n",
      "Saving models...\n",
      "[Epoch 28/100] [Batch 0/2211] [D loss: 1.558314] [G loss: 0.856135]\n",
      "[Epoch 28/100] [Batch 1000/2211] [D loss: 1.545063] [G loss: 0.852409]\n",
      "[Epoch 28/100] [Batch 2000/2211] [D loss: 1.802304] [G loss: 0.977062]\n",
      "Saving models...\n",
      "[Epoch 29/100] [Batch 0/2211] [D loss: 1.608553] [G loss: 0.860010]\n",
      "[Epoch 29/100] [Batch 1000/2211] [D loss: 1.573070] [G loss: 0.930083]\n",
      "[Epoch 29/100] [Batch 2000/2211] [D loss: 1.540346] [G loss: 0.806809]\n",
      "Saving models...\n",
      "[Epoch 30/100] [Batch 0/2211] [D loss: 1.636392] [G loss: 0.976379]\n",
      "[Epoch 30/100] [Batch 1000/2211] [D loss: 1.466891] [G loss: 0.962574]\n",
      "[Epoch 30/100] [Batch 2000/2211] [D loss: 1.572481] [G loss: 0.928423]\n",
      "Saving models...\n",
      "[Epoch 31/100] [Batch 0/2211] [D loss: 1.508146] [G loss: 0.852869]\n",
      "[Epoch 31/100] [Batch 1000/2211] [D loss: 1.558283] [G loss: 0.905183]\n",
      "[Epoch 31/100] [Batch 2000/2211] [D loss: 1.522343] [G loss: 0.745918]\n",
      "Saving models...\n",
      "[Epoch 32/100] [Batch 0/2211] [D loss: 1.576613] [G loss: 0.949327]\n",
      "[Epoch 32/100] [Batch 1000/2211] [D loss: 1.591253] [G loss: 0.961609]\n",
      "[Epoch 32/100] [Batch 2000/2211] [D loss: 1.517424] [G loss: 0.848771]\n",
      "Saving models...\n",
      "[Epoch 33/100] [Batch 0/2211] [D loss: 1.478344] [G loss: 0.934138]\n",
      "[Epoch 33/100] [Batch 1000/2211] [D loss: 1.521375] [G loss: 0.962178]\n",
      "[Epoch 33/100] [Batch 2000/2211] [D loss: 1.497895] [G loss: 0.925320]\n",
      "Saving models...\n",
      "[Epoch 34/100] [Batch 0/2211] [D loss: 1.458192] [G loss: 0.936283]\n",
      "[Epoch 34/100] [Batch 1000/2211] [D loss: 1.509959] [G loss: 0.950627]\n",
      "[Epoch 34/100] [Batch 2000/2211] [D loss: 1.512178] [G loss: 0.910387]\n",
      "Saving models...\n",
      "[Epoch 35/100] [Batch 0/2211] [D loss: 1.456580] [G loss: 0.939920]\n",
      "[Epoch 35/100] [Batch 1000/2211] [D loss: 1.577692] [G loss: 0.921683]\n",
      "[Epoch 35/100] [Batch 2000/2211] [D loss: 1.599361] [G loss: 0.965673]\n",
      "Saving models...\n",
      "[Epoch 36/100] [Batch 0/2211] [D loss: 1.594283] [G loss: 0.809356]\n",
      "[Epoch 36/100] [Batch 1000/2211] [D loss: 1.547621] [G loss: 0.984768]\n",
      "[Epoch 36/100] [Batch 2000/2211] [D loss: 1.600191] [G loss: 0.971281]\n",
      "Saving models...\n",
      "[Epoch 37/100] [Batch 0/2211] [D loss: 1.482850] [G loss: 0.926676]\n",
      "[Epoch 37/100] [Batch 1000/2211] [D loss: 1.533796] [G loss: 0.914814]\n",
      "[Epoch 37/100] [Batch 2000/2211] [D loss: 1.566396] [G loss: 0.979138]\n",
      "Saving models...\n",
      "[Epoch 38/100] [Batch 0/2211] [D loss: 1.608027] [G loss: 0.952643]\n",
      "[Epoch 38/100] [Batch 1000/2211] [D loss: 1.475835] [G loss: 0.780861]\n",
      "[Epoch 38/100] [Batch 2000/2211] [D loss: 1.545432] [G loss: 1.024200]\n",
      "Saving models...\n",
      "[Epoch 39/100] [Batch 0/2211] [D loss: 1.612226] [G loss: 0.937461]\n",
      "[Epoch 39/100] [Batch 1000/2211] [D loss: 1.663902] [G loss: 1.027875]\n",
      "[Epoch 39/100] [Batch 2000/2211] [D loss: 1.671152] [G loss: 1.061318]\n",
      "Saving models...\n",
      "[Epoch 40/100] [Batch 0/2211] [D loss: 1.483759] [G loss: 0.952692]\n",
      "[Epoch 40/100] [Batch 1000/2211] [D loss: 1.467644] [G loss: 0.961697]\n",
      "[Epoch 40/100] [Batch 2000/2211] [D loss: 1.574845] [G loss: 1.057074]\n",
      "Saving models...\n",
      "[Epoch 41/100] [Batch 0/2211] [D loss: 1.441265] [G loss: 0.854879]\n",
      "[Epoch 41/100] [Batch 1000/2211] [D loss: 1.485347] [G loss: 0.897020]\n",
      "[Epoch 41/100] [Batch 2000/2211] [D loss: 1.600046] [G loss: 0.943121]\n",
      "Saving models...\n",
      "[Epoch 42/100] [Batch 0/2211] [D loss: 1.560587] [G loss: 0.903572]\n",
      "[Epoch 42/100] [Batch 1000/2211] [D loss: 1.579107] [G loss: 0.848491]\n",
      "[Epoch 42/100] [Batch 2000/2211] [D loss: 1.564777] [G loss: 0.827438]\n",
      "Saving models...\n",
      "[Epoch 43/100] [Batch 0/2211] [D loss: 1.616215] [G loss: 0.867997]\n",
      "[Epoch 43/100] [Batch 1000/2211] [D loss: 1.545871] [G loss: 0.957402]\n",
      "[Epoch 43/100] [Batch 2000/2211] [D loss: 1.466471] [G loss: 0.894767]\n",
      "Saving models...\n",
      "[Epoch 44/100] [Batch 0/2211] [D loss: 1.537359] [G loss: 0.807905]\n",
      "[Epoch 44/100] [Batch 1000/2211] [D loss: 1.499255] [G loss: 0.822975]\n",
      "[Epoch 44/100] [Batch 2000/2211] [D loss: 1.562696] [G loss: 0.890605]\n",
      "Saving models...\n",
      "[Epoch 45/100] [Batch 0/2211] [D loss: 1.588510] [G loss: 0.967944]\n",
      "[Epoch 45/100] [Batch 1000/2211] [D loss: 1.677432] [G loss: 0.699592]\n",
      "[Epoch 45/100] [Batch 2000/2211] [D loss: 1.518253] [G loss: 0.847770]\n",
      "Saving models...\n",
      "[Epoch 46/100] [Batch 0/2211] [D loss: 1.512341] [G loss: 0.879744]\n",
      "[Epoch 46/100] [Batch 1000/2211] [D loss: 1.526359] [G loss: 0.879358]\n",
      "[Epoch 46/100] [Batch 2000/2211] [D loss: 1.560387] [G loss: 0.882679]\n",
      "Saving models...\n",
      "[Epoch 47/100] [Batch 0/2211] [D loss: 1.659914] [G loss: 0.800987]\n",
      "[Epoch 47/100] [Batch 1000/2211] [D loss: 1.527398] [G loss: 0.800986]\n",
      "[Epoch 47/100] [Batch 2000/2211] [D loss: 1.505867] [G loss: 0.934813]\n",
      "Saving models...\n",
      "[Epoch 48/100] [Batch 0/2211] [D loss: 1.480892] [G loss: 0.906795]\n",
      "[Epoch 48/100] [Batch 1000/2211] [D loss: 1.462331] [G loss: 0.839885]\n",
      "[Epoch 48/100] [Batch 2000/2211] [D loss: 1.487098] [G loss: 0.857609]\n",
      "Saving models...\n",
      "[Epoch 49/100] [Batch 0/2211] [D loss: 1.487884] [G loss: 0.894422]\n",
      "[Epoch 49/100] [Batch 1000/2211] [D loss: 1.558504] [G loss: 0.935322]\n",
      "[Epoch 49/100] [Batch 2000/2211] [D loss: 1.447598] [G loss: 0.820323]\n",
      "Saving models...\n",
      "[Epoch 50/100] [Batch 0/2211] [D loss: 1.485471] [G loss: 0.831188]\n",
      "[Epoch 50/100] [Batch 1000/2211] [D loss: 1.659920] [G loss: 1.014166]\n",
      "[Epoch 50/100] [Batch 2000/2211] [D loss: 1.643043] [G loss: 1.025905]\n",
      "Saving models...\n",
      "[Epoch 51/100] [Batch 0/2211] [D loss: 1.444826] [G loss: 0.959741]\n",
      "[Epoch 51/100] [Batch 1000/2211] [D loss: 1.493509] [G loss: 0.938432]\n",
      "[Epoch 51/100] [Batch 2000/2211] [D loss: 1.601023] [G loss: 0.859454]\n",
      "Saving models...\n",
      "[Epoch 52/100] [Batch 0/2211] [D loss: 1.443904] [G loss: 0.935777]\n",
      "[Epoch 52/100] [Batch 1000/2211] [D loss: 1.425654] [G loss: 0.798993]\n",
      "[Epoch 52/100] [Batch 2000/2211] [D loss: 1.602709] [G loss: 0.861166]\n",
      "Saving models...\n",
      "[Epoch 53/100] [Batch 0/2211] [D loss: 1.632355] [G loss: 0.868507]\n",
      "[Epoch 53/100] [Batch 1000/2211] [D loss: 1.458360] [G loss: 0.918158]\n",
      "[Epoch 53/100] [Batch 2000/2211] [D loss: 1.548994] [G loss: 0.851563]\n",
      "Saving models...\n",
      "[Epoch 54/100] [Batch 0/2211] [D loss: 1.578402] [G loss: 0.904010]\n",
      "[Epoch 54/100] [Batch 1000/2211] [D loss: 1.572409] [G loss: 0.860282]\n",
      "[Epoch 54/100] [Batch 2000/2211] [D loss: 1.622566] [G loss: 0.817854]\n",
      "Saving models...\n",
      "[Epoch 55/100] [Batch 0/2211] [D loss: 1.639661] [G loss: 0.820014]\n",
      "[Epoch 55/100] [Batch 1000/2211] [D loss: 1.551506] [G loss: 0.827850]\n",
      "[Epoch 55/100] [Batch 2000/2211] [D loss: 1.522717] [G loss: 0.810049]\n",
      "Saving models...\n",
      "[Epoch 56/100] [Batch 0/2211] [D loss: 1.606711] [G loss: 0.807862]\n",
      "[Epoch 56/100] [Batch 1000/2211] [D loss: 1.632878] [G loss: 0.815087]\n",
      "[Epoch 56/100] [Batch 2000/2211] [D loss: 1.580371] [G loss: 0.824983]\n",
      "Saving models...\n",
      "[Epoch 57/100] [Batch 0/2211] [D loss: 1.557906] [G loss: 0.800549]\n",
      "[Epoch 57/100] [Batch 1000/2211] [D loss: 1.527918] [G loss: 0.817208]\n",
      "[Epoch 57/100] [Batch 2000/2211] [D loss: 1.546767] [G loss: 0.838091]\n",
      "Saving models...\n",
      "[Epoch 58/100] [Batch 0/2211] [D loss: 1.607089] [G loss: 0.840187]\n",
      "[Epoch 58/100] [Batch 1000/2211] [D loss: 1.618096] [G loss: 0.777405]\n",
      "[Epoch 58/100] [Batch 2000/2211] [D loss: 1.602039] [G loss: 0.795850]\n",
      "Saving models...\n",
      "[Epoch 59/100] [Batch 0/2211] [D loss: 1.586804] [G loss: 0.801540]\n",
      "[Epoch 59/100] [Batch 1000/2211] [D loss: 1.568306] [G loss: 0.828714]\n",
      "[Epoch 59/100] [Batch 2000/2211] [D loss: 1.570019] [G loss: 0.817856]\n",
      "Saving models...\n",
      "[Epoch 60/100] [Batch 0/2211] [D loss: 1.643571] [G loss: 0.812765]\n",
      "[Epoch 60/100] [Batch 1000/2211] [D loss: 1.560090] [G loss: 0.823498]\n",
      "[Epoch 60/100] [Batch 2000/2211] [D loss: 1.599780] [G loss: 0.832265]\n",
      "Saving models...\n",
      "[Epoch 61/100] [Batch 0/2211] [D loss: 1.530769] [G loss: 0.817725]\n",
      "[Epoch 61/100] [Batch 1000/2211] [D loss: 1.587555] [G loss: 0.824248]\n",
      "[Epoch 61/100] [Batch 2000/2211] [D loss: 1.615330] [G loss: 0.837981]\n",
      "Saving models...\n",
      "[Epoch 62/100] [Batch 0/2211] [D loss: 1.597975] [G loss: 0.827296]\n",
      "[Epoch 62/100] [Batch 1000/2211] [D loss: 1.608891] [G loss: 0.777632]\n",
      "[Epoch 62/100] [Batch 2000/2211] [D loss: 1.660535] [G loss: 0.822019]\n",
      "Saving models...\n",
      "[Epoch 63/100] [Batch 0/2211] [D loss: 1.618923] [G loss: 0.836966]\n",
      "[Epoch 63/100] [Batch 1000/2211] [D loss: 1.662983] [G loss: 0.841233]\n",
      "[Epoch 63/100] [Batch 2000/2211] [D loss: 1.594421] [G loss: 0.812647]\n",
      "Saving models...\n",
      "[Epoch 64/100] [Batch 0/2211] [D loss: 1.578947] [G loss: 0.792902]\n",
      "[Epoch 64/100] [Batch 1000/2211] [D loss: 1.562080] [G loss: 0.816124]\n",
      "[Epoch 64/100] [Batch 2000/2211] [D loss: 1.634606] [G loss: 0.782291]\n",
      "Saving models...\n",
      "[Epoch 65/100] [Batch 0/2211] [D loss: 1.620779] [G loss: 0.817860]\n",
      "[Epoch 65/100] [Batch 1000/2211] [D loss: 1.569725] [G loss: 0.831642]\n",
      "[Epoch 65/100] [Batch 2000/2211] [D loss: 1.705209] [G loss: 0.806398]\n",
      "Saving models...\n",
      "[Epoch 66/100] [Batch 0/2211] [D loss: 1.653232] [G loss: 0.798792]\n",
      "[Epoch 66/100] [Batch 1000/2211] [D loss: 1.591977] [G loss: 0.832662]\n",
      "[Epoch 66/100] [Batch 2000/2211] [D loss: 1.550785] [G loss: 0.922386]\n",
      "Saving models...\n",
      "[Epoch 67/100] [Batch 0/2211] [D loss: 1.533271] [G loss: 0.883775]\n",
      "[Epoch 67/100] [Batch 1000/2211] [D loss: 1.526172] [G loss: 0.766414]\n",
      "[Epoch 67/100] [Batch 2000/2211] [D loss: 1.511389] [G loss: 0.925707]\n",
      "Saving models...\n",
      "[Epoch 68/100] [Batch 0/2211] [D loss: 1.539126] [G loss: 0.828527]\n",
      "[Epoch 68/100] [Batch 1000/2211] [D loss: 1.548924] [G loss: 0.765067]\n",
      "[Epoch 68/100] [Batch 2000/2211] [D loss: 1.500748] [G loss: 0.952017]\n",
      "Saving models...\n",
      "[Epoch 69/100] [Batch 0/2211] [D loss: 1.544027] [G loss: 0.965908]\n",
      "[Epoch 69/100] [Batch 1000/2211] [D loss: 1.526539] [G loss: 0.932526]\n",
      "[Epoch 69/100] [Batch 2000/2211] [D loss: 1.530369] [G loss: 0.913911]\n",
      "Saving models...\n",
      "[Epoch 70/100] [Batch 0/2211] [D loss: 1.454081] [G loss: 0.838652]\n",
      "[Epoch 70/100] [Batch 1000/2211] [D loss: 1.555737] [G loss: 0.922004]\n",
      "[Epoch 70/100] [Batch 2000/2211] [D loss: 1.501831] [G loss: 0.958304]\n",
      "Saving models...\n",
      "[Epoch 71/100] [Batch 0/2211] [D loss: 1.546934] [G loss: 0.857727]\n",
      "[Epoch 71/100] [Batch 1000/2211] [D loss: 2.107900] [G loss: 0.916150]\n",
      "[Epoch 71/100] [Batch 2000/2211] [D loss: 1.614253] [G loss: 0.759200]\n",
      "Saving models...\n",
      "[Epoch 72/100] [Batch 0/2211] [D loss: 1.508770] [G loss: 0.821269]\n",
      "[Epoch 72/100] [Batch 1000/2211] [D loss: 1.571811] [G loss: 0.892667]\n",
      "[Epoch 72/100] [Batch 2000/2211] [D loss: 1.453219] [G loss: 0.897533]\n",
      "Saving models...\n",
      "[Epoch 73/100] [Batch 0/2211] [D loss: 1.534873] [G loss: 0.928835]\n",
      "[Epoch 73/100] [Batch 1000/2211] [D loss: 1.490297] [G loss: 0.943013]\n",
      "[Epoch 73/100] [Batch 2000/2211] [D loss: 1.643341] [G loss: 0.771521]\n",
      "Saving models...\n",
      "[Epoch 74/100] [Batch 0/2211] [D loss: 1.603424] [G loss: 0.806812]\n",
      "[Epoch 74/100] [Batch 1000/2211] [D loss: 1.647919] [G loss: 0.834158]\n",
      "[Epoch 74/100] [Batch 2000/2211] [D loss: 1.624045] [G loss: 0.786027]\n",
      "Saving models...\n",
      "[Epoch 75/100] [Batch 0/2211] [D loss: 1.590006] [G loss: 0.806433]\n",
      "[Epoch 75/100] [Batch 1000/2211] [D loss: 1.605333] [G loss: 0.812639]\n",
      "[Epoch 75/100] [Batch 2000/2211] [D loss: 1.633255] [G loss: 0.845418]\n",
      "Saving models...\n",
      "[Epoch 76/100] [Batch 0/2211] [D loss: 1.538488] [G loss: 0.812124]\n",
      "[Epoch 76/100] [Batch 1000/2211] [D loss: 1.661148] [G loss: 0.768295]\n",
      "[Epoch 76/100] [Batch 2000/2211] [D loss: 1.661704] [G loss: 0.792525]\n",
      "Saving models...\n",
      "[Epoch 77/100] [Batch 0/2211] [D loss: 1.700706] [G loss: 0.764424]\n",
      "[Epoch 77/100] [Batch 1000/2211] [D loss: 1.669253] [G loss: 0.776199]\n",
      "[Epoch 77/100] [Batch 2000/2211] [D loss: 1.664351] [G loss: 0.778791]\n",
      "Saving models...\n",
      "[Epoch 78/100] [Batch 0/2211] [D loss: 1.708310] [G loss: 0.804101]\n",
      "[Epoch 78/100] [Batch 1000/2211] [D loss: 1.685883] [G loss: 0.788647]\n",
      "[Epoch 78/100] [Batch 2000/2211] [D loss: 1.637783] [G loss: 0.803610]\n",
      "Saving models...\n",
      "[Epoch 79/100] [Batch 0/2211] [D loss: 1.618166] [G loss: 0.771798]\n",
      "[Epoch 79/100] [Batch 1000/2211] [D loss: 1.640305] [G loss: 0.763884]\n",
      "[Epoch 79/100] [Batch 2000/2211] [D loss: 1.608011] [G loss: 0.798813]\n",
      "Saving models...\n",
      "[Epoch 80/100] [Batch 0/2211] [D loss: 1.672961] [G loss: 0.743421]\n",
      "[Epoch 80/100] [Batch 1000/2211] [D loss: 1.636584] [G loss: 0.806752]\n",
      "[Epoch 80/100] [Batch 2000/2211] [D loss: 1.628390] [G loss: 0.844402]\n",
      "Saving models...\n",
      "[Epoch 81/100] [Batch 0/2211] [D loss: 1.592246] [G loss: 0.823597]\n",
      "[Epoch 81/100] [Batch 1000/2211] [D loss: 1.712744] [G loss: 0.775760]\n",
      "[Epoch 81/100] [Batch 2000/2211] [D loss: 1.662983] [G loss: 0.799511]\n",
      "Saving models...\n",
      "[Epoch 82/100] [Batch 0/2211] [D loss: 1.645562] [G loss: 0.779183]\n",
      "[Epoch 82/100] [Batch 1000/2211] [D loss: 1.675462] [G loss: 0.794383]\n",
      "[Epoch 82/100] [Batch 2000/2211] [D loss: 1.625072] [G loss: 0.798146]\n",
      "Saving models...\n",
      "[Epoch 83/100] [Batch 0/2211] [D loss: 1.650593] [G loss: 0.770335]\n",
      "[Epoch 83/100] [Batch 1000/2211] [D loss: 1.631616] [G loss: 0.761096]\n",
      "[Epoch 83/100] [Batch 2000/2211] [D loss: 1.672325] [G loss: 0.765398]\n",
      "Saving models...\n",
      "[Epoch 84/100] [Batch 0/2211] [D loss: 1.651401] [G loss: 0.784326]\n",
      "[Epoch 84/100] [Batch 1000/2211] [D loss: 1.669719] [G loss: 0.721585]\n",
      "[Epoch 84/100] [Batch 2000/2211] [D loss: 1.643713] [G loss: 0.762303]\n",
      "Saving models...\n",
      "[Epoch 85/100] [Batch 0/2211] [D loss: 1.623396] [G loss: 0.776152]\n",
      "[Epoch 85/100] [Batch 1000/2211] [D loss: 1.617480] [G loss: 0.760665]\n",
      "[Epoch 85/100] [Batch 2000/2211] [D loss: 1.613911] [G loss: 0.797528]\n",
      "Saving models...\n",
      "[Epoch 86/100] [Batch 0/2211] [D loss: 1.705908] [G loss: 0.756537]\n",
      "[Epoch 86/100] [Batch 1000/2211] [D loss: 1.637515] [G loss: 0.766376]\n",
      "[Epoch 86/100] [Batch 2000/2211] [D loss: 1.658536] [G loss: 0.780841]\n",
      "Saving models...\n",
      "[Epoch 87/100] [Batch 0/2211] [D loss: 1.641438] [G loss: 0.765129]\n",
      "[Epoch 87/100] [Batch 1000/2211] [D loss: 1.644405] [G loss: 0.792433]\n",
      "[Epoch 87/100] [Batch 2000/2211] [D loss: 1.610285] [G loss: 0.833177]\n",
      "Saving models...\n",
      "[Epoch 88/100] [Batch 0/2211] [D loss: 1.735906] [G loss: 0.882351]\n",
      "[Epoch 88/100] [Batch 1000/2211] [D loss: 1.639509] [G loss: 0.807971]\n",
      "[Epoch 88/100] [Batch 2000/2211] [D loss: 1.604621] [G loss: 0.819380]\n",
      "Saving models...\n",
      "[Epoch 89/100] [Batch 0/2211] [D loss: 1.618280] [G loss: 0.761580]\n",
      "[Epoch 89/100] [Batch 1000/2211] [D loss: 1.627098] [G loss: 0.796421]\n",
      "[Epoch 89/100] [Batch 2000/2211] [D loss: 1.629289] [G loss: 0.776888]\n",
      "Saving models...\n",
      "[Epoch 90/100] [Batch 0/2211] [D loss: 1.613676] [G loss: 0.783865]\n",
      "[Epoch 90/100] [Batch 1000/2211] [D loss: 1.637369] [G loss: 0.788130]\n",
      "[Epoch 90/100] [Batch 2000/2211] [D loss: 1.630880] [G loss: 0.805380]\n",
      "Saving models...\n",
      "[Epoch 91/100] [Batch 0/2211] [D loss: 1.616776] [G loss: 0.770108]\n",
      "[Epoch 91/100] [Batch 1000/2211] [D loss: 1.601486] [G loss: 0.828302]\n",
      "[Epoch 91/100] [Batch 2000/2211] [D loss: 1.597848] [G loss: 0.807365]\n",
      "Saving models...\n",
      "[Epoch 92/100] [Batch 0/2211] [D loss: 1.630789] [G loss: 0.761198]\n",
      "[Epoch 92/100] [Batch 1000/2211] [D loss: 1.587184] [G loss: 0.797072]\n",
      "[Epoch 92/100] [Batch 2000/2211] [D loss: 1.649090] [G loss: 0.817241]\n",
      "Saving models...\n",
      "[Epoch 93/100] [Batch 0/2211] [D loss: 1.602149] [G loss: 0.783089]\n",
      "[Epoch 93/100] [Batch 1000/2211] [D loss: 1.628571] [G loss: 0.791440]\n",
      "[Epoch 93/100] [Batch 2000/2211] [D loss: 1.614058] [G loss: 0.819455]\n",
      "Saving models...\n",
      "[Epoch 94/100] [Batch 0/2211] [D loss: 1.615544] [G loss: 0.808795]\n",
      "[Epoch 94/100] [Batch 1000/2211] [D loss: 1.623970] [G loss: 0.801705]\n",
      "[Epoch 94/100] [Batch 2000/2211] [D loss: 1.619565] [G loss: 0.766791]\n",
      "Saving models...\n",
      "[Epoch 95/100] [Batch 0/2211] [D loss: 1.620369] [G loss: 0.783837]\n",
      "[Epoch 95/100] [Batch 1000/2211] [D loss: 1.649824] [G loss: 0.779058]\n",
      "[Epoch 95/100] [Batch 2000/2211] [D loss: 1.618104] [G loss: 0.823251]\n",
      "Saving models...\n",
      "[Epoch 96/100] [Batch 0/2211] [D loss: 1.641173] [G loss: 0.825665]\n",
      "[Epoch 96/100] [Batch 1000/2211] [D loss: 1.655861] [G loss: 0.828484]\n",
      "[Epoch 96/100] [Batch 2000/2211] [D loss: 1.643881] [G loss: 0.759742]\n",
      "Saving models...\n",
      "[Epoch 97/100] [Batch 0/2211] [D loss: 1.638571] [G loss: 0.809872]\n",
      "[Epoch 97/100] [Batch 1000/2211] [D loss: 1.613175] [G loss: 0.770177]\n",
      "[Epoch 97/100] [Batch 2000/2211] [D loss: 1.657216] [G loss: 0.776389]\n",
      "Saving models...\n",
      "[Epoch 98/100] [Batch 0/2211] [D loss: 1.663009] [G loss: 0.778521]\n",
      "[Epoch 98/100] [Batch 1000/2211] [D loss: 1.665814] [G loss: 0.802385]\n",
      "[Epoch 98/100] [Batch 2000/2211] [D loss: 1.617198] [G loss: 0.808918]\n",
      "Saving models...\n",
      "[Epoch 99/100] [Batch 0/2211] [D loss: 1.592758] [G loss: 0.820845]\n",
      "[Epoch 99/100] [Batch 1000/2211] [D loss: 1.723633] [G loss: 0.734663]\n",
      "[Epoch 99/100] [Batch 2000/2211] [D loss: 1.595359] [G loss: 0.738153]\n",
      "Saving models...\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (data, labels) in enumerate(dataloader):\n",
    "        batch_size = data.shape[0]\n",
    "        real_data = Variable(data.type(FloatTensor))        \n",
    "        labels = Variable(labels.float())\n",
    "        labels = labels.view(-1, 1).type(FloatTensor)\n",
    "        \n",
    "        # Train the discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        d_real_outputs = discriminator(real_data)\n",
    "        d_real_loss = adversarial_loss(d_real_outputs, labels)\n",
    "        \n",
    "        fake_data = generator(noise, 0)\n",
    "        d_fake_outputs = discriminator(fake_data.detach())\n",
    "        d_fake_loss1 = adversarial_loss(d_fake_outputs, torch.full((batch_size, 1), 0.4).type(FloatTensor))\n",
    "        \n",
    "        fake_data = generator(noise, 1)\n",
    "        d_fake_outputs = discriminator(fake_data.detach())\n",
    "        d_fake_loss2 = adversarial_loss(d_fake_outputs, torch.full((batch_size, 1), 0.6).type(FloatTensor))\n",
    "        \n",
    "        d_loss = d_real_loss + d_fake_loss1 + d_fake_loss2\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # Train the generator\n",
    "        optimizer_G.zero_grad()\n",
    "        noise = torch.randn(batch_size, latent_dim)\n",
    "        fake_data = generator(noise, 0)\n",
    "        d_fake_outputs = discriminator(fake_data)\n",
    "        targets = torch.full((batch_size, 1), 0)\n",
    "        g_loss1 = adversarial_loss(d_fake_outputs, targets.float())\n",
    "\n",
    "        noise = torch.randn(batch_size, latent_dim)\n",
    "        fake_data = generator(noise, 1)\n",
    "        d_fake_outputs = discriminator(fake_data)\n",
    "        targets = torch.full((batch_size, 1), 1)\n",
    "        g_loss2 = adversarial_loss(d_fake_outputs, targets.float())\n",
    "        \n",
    "        g_loss = g_loss1 + g_loss2\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "\n",
    "        # Print the loss for each epoch\n",
    "        if i % 1000 == 0:\n",
    "            print(\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\" % (epoch, num_epochs, i, len(dataloader), d_loss.item(), g_loss.item()))\n",
    "\n",
    "    print('Saving models...')\n",
    "    # save discriminator model used for classification\n",
    "    PATH_disc = './models/discr_model-'\n",
    "    PATH = PATH_disc + 'lr=' + str(lr) + '-batch=' + str(batch_size) + '-epochs=' + str(epoch) + '.pth'\n",
    "    torch.save(discriminator.state_dict(), PATH)\n",
    "    \n",
    "    # save generator model\n",
    "    PATH_gen = './models/gen_model-'\n",
    "    PATH = PATH_gen + 'lr=' + str(lr) + '-batch=' + str(batch_size) + '-epochs=' + str(epoch) + '.pth'\n",
    "    torch.save(generator.state_dict(), PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
